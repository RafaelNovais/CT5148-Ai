{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOXPJjN8OdpgOaS49mxwOT7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RafaelNovais/MasterAI/blob/master/MindMapSLDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a480rTUr9PSl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Learning outcomes\n",
        "• LO1: Be able to define large-scale data analytics and understand its characteristics\n",
        "\n",
        "    1.   Distributed Sources data on the web\n",
        "    2.   Processed using diistributed and parallel computing approaches\n",
        "    3.   Distributed processing using multiple networked machines\n",
        "    \n",
        "\n",
        "\n",
        "• LO2: Be able to explain and apply concepts and tools for distributed and parallel processing of large-scale data\n",
        "\n",
        "    1.   Concurrency means that two or more computing processes or threads can run in an unordered, partially ordered or overlapping way, without affecting the overall result\n",
        "    2.   Concurrency can happen on the level of entire processes, or within a single process on the thread-level (multithreading; concurrency of threads)\n",
        "\n",
        "\n",
        "\n",
        "• LO3: Know how to explain and apply concepts and tools for highly scalable collection, querying,\n",
        "filtering, sorting and synthesizing of data\n",
        "\n",
        "\n",
        "• LO4: Know how to describe and apply selected statistical and machine learning techniques and tools\n",
        "for the analysis of large-scale data\n",
        "\n",
        "\n",
        "• LO5: Know how to explain and apply approaches to stream data analytics and complex event\n",
        "processing\n",
        "\n",
        "\n",
        "• LO6: Understand and be able to discuss privacy issues in connection with large-scale data analytics\n",
        "\n",
        "\n",
        "Planned for the rest of the semester\n",
        "• Revision of advanced Java programming concepts (mainly parallel computing)\n",
        "• Relevant new Java 8 concepts (e.g., Java 8 streams, Lambda expressions)\n",
        "  \n",
        "#  Functional programming\n",
        "\n",
        "    • Avoid mutation (in-place modification of existing data instead of reating a new (modified object), where possible\n",
        "    • Avoid global variables (e.g., public static fields)\n",
        "    • Avoid side effects of functions. The results of functions/methods should ideally only depend on their arguments. Methods/functions should not   manipulate shared state, as far as possible.\n",
        "    • Where possible, rely on inherently \"parallel\" data structures instead of designing parallel solutions manually (e.g., use parallel streams with Java8\n",
        "\n",
        "\n",
        "# Anonymous classes\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Lambda expressions\n",
        "\n",
        "    \n",
        "    *Lambda expressions are anonymous functions (functions without names)\n",
        "    *(int x, int y) -> x + y\n",
        "    *Parameters are optional. If there are no parameter, write () before the arrow\n",
        "    *Lambda expressions can use arbitrary blocks of code to produce a result\n",
        "    *You can provide a lambda expression everywhere where an object with a matching functional interface as type is expected\n",
        "    *\n",
        "\n",
        "\n",
        "```\n",
        "Ex:\n",
        "(int x, int y) -> x + y\n",
        "\n",
        "x -> {\n",
        "double d = 5.0;\n",
        "System.out.println(\"x: \" + x);\n",
        "double dx = d * x;\n",
        "return dc - 7.2;\n",
        "}\n",
        "\n",
        "```\n",
        "\n",
        "# Synchronized:\n",
        "Very general and good performance (if used correctly) but error-prone. Can degrade performance if used incorrectly (over-blocking):\n",
        "\n",
        "\n",
        "# Deadlock\n",
        "\n",
        "\n",
        "\n",
        "# ParallelStream\n",
        "\n",
        "\n",
        "# MapReduce\n",
        "\n",
        "    * MapReduce makes only sense if operations can be distributed across multiple machines and multiple cores on individual machines\n",
        "    * Cluster of machines, multi- and many-core computing (including multithreading and GPUs)\n",
        "    * Analogously for reduce shuffle\n",
        "\n",
        "    \n",
        "*  The Data Analytics pipeline\n",
        "* The MapReduce approach\n",
        "* Introduction to cluster computing and Hadoop\n",
        "* Introduction to Apache Spark\n",
        "* Working with RDDs (Resilient Distributed Datasets)\n",
        "* Spark Data Frames and Data Sets\n",
        "* Obtaining statistics over data\n",
        "* Representing and storing data\n",
        "* Deployment of LSDA tasks on a cluster of computers\n",
        "* Basics of parallel and clustered machine learning\n",
        "* Machine Learning with Spark (e.g., for classification)\n",
        "* Stream and event data analytics\n",
        "\n",
        "#Spark\n",
        "* Basic distributed data structures:\n",
        "* RDDs (Resilient Distributed Datasets): easy to use, intuitive, stable and flexible - but not the most efficient, i.a. due to large overhead for serialization)  Stored in RAM, Immutable , Distributed storage and processing\n",
        "              JavaRDD<String> pessoasComMaisDe30RDD = dadosRDD.filter(s -> {\n",
        "            int idade = Integer.parseInt(s.split(\",\")[1]);\n",
        "            return idade > 30;\n",
        "        });\n",
        "\n",
        "\n",
        "* DataFrame (since Spark 1.3): fast, stable, close to SQL (not natural to use for non-SQL experts), restricted expressiveness compared to RDDs, works best with Scala\n",
        "       Dataset<Row> pessoasComMaisDe30DF = dadosDF.filter(\"Idade > 30\");\n",
        "\n",
        "* Datasets (since Spark 1.6 as a preview, fully supported since Spark 2.0): sort of mix between RDDs and DataFrames, fast but not fully stable yet\n",
        "* We will start with RDDs (because they are conceptually close to Java 8 Streams\n",
        "and partially underlying DataFrames/sets) and later also cover DataFrames and\n",
        "Datasets\n",
        "*k-Means Clustering\n",
        "Easy to understand, and typically k-means is used as a default approach before other\n",
        "approaches are considered. Can be hard to compute without auxiliary heuristics (which are\n",
        "not covered in this module) but in practice it is often very fast\n",
        "\n",
        "\n",
        "# Descriptions of some of them on the following slides\n",
        "```\n",
        "• map\n",
        "• filter\n",
        "• groupBy\n",
        "• sort\n",
        "• union\n",
        "• join\n",
        "• leftOuterJoin\n",
        "• rightOuterJoin\n",
        "• flatMap\n",
        "• reduce\n",
        "• count\n",
        "• fold\n",
        "• reduceByKey\n",
        "• groupByKey\n",
        "• cogroup\n",
        "• cross\n",
        "• zip\n",
        "• cartesian\n",
        "• sample\n",
        "• take\n",
        "• first\n",
        "• partitionBy\n",
        "• mapWith\n",
        "• pipe\n",
        "• distinct\n",
        "• save\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "84WI01qHGkQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Same example using Spark and Scala (including parsing)\n",
        "val textFile = spark.textFile(\"hdfs://...\")\n",
        "val counts = textFile.flatMap(line => line.split(\" \"))\n",
        ".map(word => (word, 1))\n",
        ".reduceByKey(_ + _)\n",
        "counts.saveAsTextFile(\"hdfs://...\")\n"
      ],
      "metadata": {
        "id": "-aUN_2Wno3qG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mapper Function:\n",
        "**Input**:\n",
        "Each line of the text.\n",
        "**Output**:\n",
        "Key-Value pairs where the key is the word and the value is 1.\n",
        "For example, for the line \"C precedes C++\", the mapper will output:\n",
        "\n",
        "# Reducer Function:\n",
        "**Input**:\n",
        "Key-Value pairs from the mapper where keys are words and values are lists of 1s.\n",
        "**Output**:\n",
        "Key-Value pairs where the key is the word and the value is the sum of occurrences.\n",
        "For example, for the input:\n",
        "\n",
        "# MapReduce Process:\n",
        "#Map Phase:\n",
        "Each mapper processes one line of the text.\n",
        "It tokenizes the line into words.\n",
        "For each word, it emits a key-value pair where the word is the key and the value is 1.\n",
        "#Shuffle and Sort:\n",
        "The output of the mappers is shuffled and sorted based on keys to group together the key-value pairs with the same key.\n",
        "#Reduce Phase:\n",
        "Each reducer receives a key along with the list of values associated with that key.\n",
        "It sums up the occurrences of the word by adding the values together.\n",
        "It emits a key-value pair where the key is the word and the value is the total count of occurrences.\n",
        "**Final Output:**\n",
        "After the MapReduce job is completed, the final output will be a list of key-value pairs where the key is each unique word in the text and the value is the total count of occurrences of that word. For example:"
      ],
      "metadata": {
        "id": "pYUKRt8VC5Ya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Spark code for classification of news headlines:\n",
        "\n",
        "\n",
        "public static void main(String[] args) {\n",
        "    SparkConf sparkConf = new SparkConf()\n",
        "            .setAppName(\"HeadlinesClassification\")\n",
        "            .setMaster(\"local[4]\")\n",
        "            .set(\"spark.executor.memory\", \"16g\");\n",
        "    JavaSparkContext sc = new JavaSparkContext(sparkConf);\n",
        "\n",
        "    // Load data from file\n",
        "    JavaRDD<String> lines = sc.textFile(\"headlines.txt\");\n",
        "\n",
        "    // Initialize HashingTF\n",
        "    final HashingTF tf = new HashingTF();\n",
        "\n",
        "    // Process training data\n",
        "    JavaRDD<String> trainingDataLines = lines.filter(line -> !line.startsWith(\"test:\"));\n",
        "    JavaRDD<LabeledPoint> trainingData = trainingDataLines.map(line -> {\n",
        "        String[] parts = line.split(\",\");\n",
        "        double label = Double.parseDouble(parts[0]);\n",
        "        String text = parts[1];\n",
        "        return new LabeledPoint(label, tf.transform(Arrays.asList(text.split(\" \"))));\n",
        "    });\n",
        "\n",
        "    // Train SVM model\n",
        "    final SVMModel model = SVMWithSGD.train(trainingData.rdd(), 100);\n",
        "\n",
        "    // Prepare test data\n",
        "    String testHeadline = lines.filter(line -> line.startsWith(\"test:\")).first().substring(5);\n",
        "    Vector testHeadlineVector = tf.transform(Arrays.asList(testHeadline.split(\" \")));\n",
        "\n",
        "    // Predict using the model\n",
        "    double prediction = model.predict(testHeadlineVector);\n",
        "\n",
        "    // Output prediction\n",
        "    System.out.println(\"Prediction for test headline: \" + prediction);\n",
        "\n",
        "    sc.stop();\n",
        "    sc.close();\n",
        "}\n"
      ],
      "metadata": {
        "id": "n9EIh1sQYOTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1 - Often, large numbers of inexpensive standard hardware devices (instead of small numbers of high-performance machines) are used to process “Big Data” (very large amounts of data). Which advantages and possible disadvantages of this approach do you see?\n",
        "\n",
        "#Advantages:\n",
        "1. Cost-Effectiveness\n",
        "\n",
        "    Standard hardware devices are generally cheaper compared to high-performance machines. Using them in large numbers can significantly reduce the overall cost of the infrastructure.\n",
        "2. Scalability:\n",
        "\n",
        "    This approach allows for easy scalability. As the volume of data grows, more inexpensive hardware devices can be added to the cluster to handle the increased workload.\n",
        "3. Fault Tolerance:\n",
        "\n",
        "    With a distributed setup of inexpensive hardware, the system becomes inherently fault-tolerant. If one machine fails, the impact on the overall system is minimal as the workload can be distributed across other functioning nodes.\n",
        "4. Flexibility:\n",
        "\n",
        "    Standard hardware devices offer flexibility in terms of vendor choices and configurations. Organizations can choose hardware based on their specific requirements and budget constraints.\n",
        "5. Parallel Processing:\n",
        "\n",
        "    Large numbers of inexpensive hardware devices enable parallel processing of data, which can significantly improve the performance of Big Data processing tasks.\n",
        "\n",
        "# Disadvantages:\n",
        "1. Management Complexity:\n",
        "\n",
        "    Managing a cluster of numerous inexpensive hardware devices can be complex. It requires expertise in cluster management, configuration, monitoring, and maintenance.\n",
        "2. Higher Failure Rate:\n",
        "\n",
        "    Inexpensive hardware devices may have a higher failure rate compared to high-performance machines. This increases the likelihood of hardware failures within the cluster, necessitating robust fault-tolerance mechanisms.\n",
        "3. Performance Variability:\n",
        "\n",
        "    Standard hardware devices may exhibit variability in performance due to differences in specifications, configurations, and quality. Ensuring consistent performance across all nodes in the cluster can be challenging.\n",
        "4. Limited Resources per Node:\n",
        "\n",
        "    Inexpensive hardware devices typically have lower computational power, memory, and storage capacities compared to high-performance machines. This limitation may restrict the types of Big Data processing tasks that can be efficiently performed on individual nodes.\n",
        "5. Network Bottlenecks:\n",
        "\n",
        "    A large cluster of inexpensive hardware devices requires efficient networking infrastructure to ensure smooth communication and data transfer between nodes. Inadequate network bandwidth or latency issues can become significant bottlenecks in the overall system performance.\n",
        "6. Power and Space Requirements:\n",
        "\n",
        "    Managing a large number of hardware devices consumes more power and requires more physical space compared to a smaller number of high-performance machines. This can result in increased operational costs and infrastructure complexity."
      ],
      "metadata": {
        "id": "yA1vgKYrZlN6"
      }
    }
  ]
}