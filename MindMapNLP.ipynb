{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOrqTxvL/sDGBD6/mfVJP2V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RafaelNovais/MasterAI/blob/master/MindMapNLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Week 1\n",
        "\n",
        "# **Natural Language Processing (NLP)**\n",
        "\n",
        "* Classification\n",
        "* Annotation\n",
        "* Parsing\n",
        "* Translation\n",
        "\n",
        "**Natural Language Generation**\n",
        "\n",
        "**Knowledge Extraction**\n",
        "\n",
        "**Speech Processing and Multimodal NLP**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# **Human Language**\n",
        "\n",
        "* Human Language as a Social Tool\n",
        "* Human Language as a Biological System\n",
        "* Human Language as a Logical System\n",
        "* Human Language and Politics\n",
        "\n",
        "Language Families, Lexical Ambiguity , Syntactic Ambiguity\n",
        "\n",
        "\n",
        "**Language Data**\n",
        "\n",
        "\n",
        "* Types\n",
        "* Some Specifics by Type\n",
        "* Corpus +Corpora A corpus may contain texts in a single language (monolingual corpus) or text data in multiple languages (multilingual corpus). In order to teach the computer to model (‘understand’) how a language works.\n",
        "* Annotated Corpus Corpus with annotations on word, phrase, sentence or document level is used in NLP for supervised training and/or evaluation\n",
        "* General Corpus Representative of a language as a whole, Used for training general purpose NLP tools/models ex:Corpus of Contemporary American English (COCA)\n",
        "* Domain Corpus Representative of a domain-specific subset of a language, Used for training domain-specific NLP tools/models, e.g. language as used specifically, e.g., in healthcare, finance, legal,\n",
        "* Monolingual Corpus in one language\n",
        "* Multilingual Corpus in two or more languagesv\n",
        "* Parallel Corpus ollection of translated documents, e.g. proceedings of the European Parliament with translations between 21 European languages\n",
        "* Comparable Corpus collection of documents on the same topic in different languages, e.g. links between Wikipedia articles in different languages\n",
        "* Multimodal Corpus Consists not only of language data but also image, audio, video\n",
        "\n",
        "\n",
        "**Language Data - Lexicon**\n",
        "\n",
        "Database of words with lexical properties such as spelling, pronunciation, morphology, PoS, semantics, Originating out of paper dictionaries\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3YGk_48FirXS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Week 2\n",
        "\n",
        "# Linguistics and NLP\n",
        "\n",
        "**Linguistic study develops a theory that explains**\n",
        "* use of language - ‘language performance’\n",
        "* knowledge and acquisition of language - ‘language competence’\n",
        "\n",
        "**NLP is concerned with**\n",
        "* use of language - data\n",
        "* knowledge and acquisition of language - models\n",
        "\n",
        "**Linguistic Levels of Analysis**\n",
        "\n",
        "* Phonology  – acoustic signal       \t\t\t - Sound\n",
        "* Morphology – word                 \t     - structure\n",
        "* Syntax     – sentence and phrase    \t\t - structure\n",
        "* Semantics  – meaning and reference \t\t\t - Meaning\n",
        "* Discourse  – relations between sentences - Meaning\n",
        "* Pragmatics – communication               - Meaning\n",
        "\n",
        "**Tokenization**\n",
        "* Tokenization is the process of dividing a piece of text into ‘tokens’\n",
        "* The number of tokens is the total occurrence of individual words\n",
        "* The number of types is the total occurrence of unique words\n",
        "* Non-linguistic tokens which are used in tweets e.g. hashtags, mentions,emoticons.\n",
        "* Compound: concatenation of simple words into a complex word.\n",
        "    bedroom > bed + room\n",
        "\n",
        "```\n",
        "For example, in the sentence:\n",
        "‘the cat sat on the mat’\n",
        "there are:\n",
        "6 tokens = the, cat, sat, on, the, mat  \n",
        "5 types  = the, cat, sat, on, mat\n",
        "```\n",
        "\n",
        "**Morphology**\n",
        "* Stemming: Stemming is the non-linguistic analysis of word structure by removing endings or beginnings of words - leaving a common stem:\n",
        "\n",
        "  *Learning > learn-ing*\n",
        "\n",
        "* Lemmatization: is the linguistic analysis of word structure by way of a\n",
        "transformation of morphologically related words into a common lemma:\n",
        "\n",
        "  *studies ies > y study*\n",
        "\n",
        "* Inflection: Linguistic analysis of internal structure of words into\n",
        "lemma with inflectional features\n",
        "```\n",
        "For example:\n",
        "writes write + Verb + 3rd Person + Singular + Present\n",
        "wrote write + Verb + 3rd Person + Singular + Past\n",
        "writing write + Verb + Progressive\n",
        "```\n",
        "\n",
        "* Derivation: Linguistic analysis of internal structure of words into\n",
        "root word + derivational affix (suffix/infix/prefix)\n",
        "```\n",
        "For example:\n",
        "agreement noun - derived from - verb agree + ment\n",
        "friendship noun - derived from - noun friend + ship\n",
        "civilize verb - derived from - adjective civil + ize\n",
        "civilization noun - derived from - verb civilize+ (e>a)tion\n",
        "```\n",
        "\n",
        "\n",
        "**Syntax**\n",
        "\n",
        "Study of the structure of sentences and how words modify one another.\n",
        "We want to understand how sentences are formed from words in a\n",
        "certain language\n",
        "\n",
        "* Part of Speech (PoS)\n",
        "PoS tag: syntactic category of a word (noun, verb, etc.)\n",
        "Why do we need it? Useful in semantic analysis:\n",
        "\n",
        "```\n",
        "‘are able to’\n",
        "They can fish\n",
        "modal_verb\n",
        "\n",
        "‘put into cans’can:\n",
        "They can fish\n",
        "verb\n",
        "```\n",
        "\n",
        "**Semantics**\n",
        "\n",
        "* The study of meaning (interpretation) and reference\n",
        "* Lexical Semantics - meaning of a word (table, large, to book, in, ...)\n",
        "* Compositional Semantics - combined meaning of several words in a phrase (large table, book a table) or sentence (He booked a large table.)\n",
        "* Discourse Semantics - combined meaning of several sentences (He booked a table. It was still available.) Note: also in dialog (‘turns’ in chatbots and other dialog systems)\n",
        "* Foundations in Semiotics: the study of signs, symbols and their reference\n",
        "    1. objects in the world (referents)\n",
        "    2. cognitive interpretation of these objects (concepts)\n",
        "    3. symbolic representations for these cognitive interpretations (symbols)\n",
        "    4. Ex Cerebro > Gato (Animal) > Word \"Cat\"\n",
        "\n",
        "\n",
        "**Approaches to WSD**\n",
        "* Unsupervised\n",
        "  1. Knowledge-based - *Lesk algorithm*\n",
        "  2. Graph-based - *Graph Distance*\n",
        "* Supervised\n",
        "  1. Word Sense Annotated Corpora - *Nearest-Neighbor Algorithm*\n",
        "* Semi-supervised\n",
        "  1. Distant supervision - *We do not consider labeled data in a corpus but use related data at a ‘distance’ as ‘weakly labeled data’*\n",
        "\n",
        "**Semantic Role Labelling**\n",
        "\n",
        "A parte resposnavel por entender a acao.\n",
        "Process of understanding who did what to whom, with what, where ... NLP task of assigning labels to entities (expressed by words or phrases) indicating their\n",
        "semantic role in an event, for example: several entities with different semantic roles in a hitting event:\n",
        "\n",
        "**Coreference Resolution**\n",
        "\n",
        "Task of identifying words and/or phrases referring to the same entity\n",
        "```\n",
        "Barack Obama (1) nominated Hillary Clinton (2) as His (1) secretary, He (1) chose Her (2) because She (2) has exp.\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4z9mRjQif3Kx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Week 3\n",
        "\n",
        "# Vector Space Model\n",
        "\n",
        "* Document Set\n",
        "* Index Terms\n",
        "* Inverted Index - Term-Document matrix (alphabetically sorted)\n",
        "* Document Vectors\n",
        "* Document Retrieval\n",
        "* Vector Space\n",
        "* Term Vectors\n",
        "* Term Vector Space\n",
        "\n",
        "**Latent Semantics**\n",
        "* Represent index terms by their distribution over documents\n",
        "* Index terms with similar meaning will show similarity in distributions\n",
        "* ‘Latent Semantic Analysis’ (LSA) or ‘Latent Semantic Indexing’ (LSI)\n",
        "\n",
        "\n",
        "**Distributional Semantics**\n",
        "* Can we take the idea of Latent Semantics but consider words as features\n",
        "instead of documents\n",
        "* Distributional Semantics can be seen as a generalization of Latent\n",
        "Semantics for the semantic representation of words in context\n",
        "* Distributional Hypothesis - *cat and dog occur in similar contexts, therefore similar in meaning*\n",
        "\n",
        "Semantic Similarity\n",
        "```\n",
        "Compute similarity of two words using their vector representations\n",
        "Compute cosine of angle between two vectors\n",
        "Cosine similarity (or ‘cosine distance’)\n",
        "```\n",
        "\n",
        "**Context in Distributional Representations**\n",
        "* Context window - document, sentence, phrase, word sequence\n",
        "* Context content - stopwords, specific part-of-speech, syntactic dependencies, etc.\n",
        "* Context weights - measure of co-occurrence strength between target and context words\n",
        "\n",
        "```\n",
        "Binary\n",
        "  the value of vector for dimension c is 1 if context c occurs with word w and 0 otherwise\n",
        "Frequency\n",
        "  the value of vector for dimension c is the number of times that context c occurs with word w\n",
        "Weight (Relevance, Specificity)\n",
        "  the value of vector for dimension c is a weight, expressing how relevant or specific context c is for word w\n",
        "```\n",
        "\n",
        "**Context Weight - TF-IDF**\n",
        "\n",
        "```\n",
        "Term Frequency - Inverse Document Frequency (TF-IDF)\n",
        "* TF-IDF measures importance of a Term in a Document where it occurs, in comparison to the overall occurrence of this Term in a set of Documents.\n",
        "* TF-IDF mostly used in IR to put weights on index terms but used also in Distributional Semantics, using words (as Terms) and sentences (as Docs):\n",
        "    TF-IDF(w) = TF(w) * IDF(w)\n",
        "    TF(w) = frequency of word w in sentence s\n",
        "    IDF(w) = log (all sentences / all sentences with w)\n",
        "```\n",
        "\n",
        "**Context Weight - PMI**\n",
        "\n",
        "```\n",
        "Pointwise Mutual Information (PMI) is a correlation or association measure that quantifies the likelihood of co-occurrence of two events\n",
        "In Distributional Semantics we use PMI to measure the likelihood of co-occurrence of two words across a corpus:\n",
        "    PMI (w1 , w2) = log ( P(w1 , w2) / P(w1) * P(w2))\n",
        "    P(w1 , w2) = probability of w1 co-occurring with w2 in corpus C\n",
        "    P(w1) P(w2) = probability (relative frequency) of w1 w2 occurring in C\n",
        "    = frequency of w1 w2 occurring in C / all words W in C\n",
        "```\n",
        "\n",
        "\n",
        "# Distributional Semantics in Action  \n",
        "\n",
        "*is an approach to understanding the meanings of words and phrases based on the idea that words that appear in similar contexts tend to have similar meanings. It focuses on the distribution of words across large bodies of text and leverages the patterns of word co-occurrence to infer semantic relationships.*\n",
        "\n",
        "\n",
        "**Word Embeddings**\n",
        "\n",
        "\n",
        "* Generalization\n",
        "\n",
        "```\n",
        "● Capture similarity on the level of word vectors rather than words Prediction\n",
        "● Predict words and their contexts by use of language modeling Dimensionality Reduction\n",
        "● Optimize sparse vectors (with lots of zeros) into shorter dense vectors\n",
        "```\n",
        "\n",
        "* Vectorization\n",
        "\n",
        "```\n",
        "Machine Learning (also deep learning) algorithms operate on numbers, not on textual data.\n",
        "To convert textual data into a numerical representation we can use vectorization:\n",
        "  ● Bag Of Words (frequency)\n",
        "  \n",
        "  ● TF-IDF, PMI (relative frequency, probability)\n",
        "    Term Frequency - Inverse Document Frequency (TF-IDF) is a measure that quantifies the importance of a word across a corpus\n",
        "\n",
        "  ● Prediction (language models)\n",
        "    Predict co-occurrence contexts instead of counting\n",
        "    Use language models to assign a co-occurrence probability to a sequence of words\n",
        "    Predicts probability of a word given a context (sequence of previous words)\n",
        "\n",
        "```\n",
        "\n",
        "# Large Language Models (LLM)\n",
        "\n",
        "**LLM Techniques for NLP**\n",
        "\n",
        "* *Fine-tuning:* updating a model’s parameter weights to support inference on a new task by fine-tuning a pre-trained LLM on the target task.\n",
        "* *Multi-task Learning:* training a model on multiple tasks simultaneously in order to produce a robust and generalizable model that is effective across multiple tasks.\n",
        "* *Transfer Learning:* taking a model that has been pre-trained or fine-tuned on previous tasks and further training it on a new task with the goal of transferring previous knowledge to the new task.\n",
        "* *Few-shot Learning:* train a model to perform a target task using a smaller\n",
        "training set (e.g. 2-10 examples).\n",
        "* *Zero-Shot Learning:* induce the model to support inference on an unseen task without any training examples or using 1-2 example by use of prompting.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "72gI2mqEzNA1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Textual Data Preprocessing\n",
        "\n",
        "```\n",
        "Normalize textual data in regard to tokenization, morphological variation Optional steps, depending on application domain, task, etc.\n",
        "  ● remove punctuation\n",
        "  ● remove or transform tokens with special characters (social media).\n",
        "  ● normalize to lowercase, truecase\n",
        "  ● remove stop words (prepositions, determiners, ...)\n",
        "  ● stemming and/or lemmatization\n",
        "  ● identify multi-word expressions\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "KsaF9VNKb9ZK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Week 4\n",
        "\n",
        "# Text Classification\n",
        "\n",
        "* Sentiment Analysis\n",
        "* Subject Classification\n",
        "* Suggestion Mining\n",
        "\n",
        "```\n",
        "Suggestion Mining comprises:\n",
        "• suggestion classification – binary (y/n) classification of input string\n",
        "• suggestion extraction – span identification of the suggestion\n",
        "• suggestion aspect detection – identify the aspect of the suggestion\n",
        "• suggestions clustering – aggregate similar suggestions together (might include paraphrasing)\n",
        "```\n",
        "\n",
        "**Bayes’ Law and Naïve Bayes**\n",
        "```\n",
        "We assume that:\n",
        "P(F1∩F2∩F3|Pos) = P(F1|Pos)P(F2|Pos)P(F3|Pos)\n",
        "```\n",
        "\n",
        "**Learning Bayesian Models**\n",
        "```\n",
        "F-Measure\n",
        "We would like to combine precision and recall\n",
        "We can always achieve 100% recall or 100% precision.\n",
        "\n",
        "Always predict true or false\n",
        "So we can’t just use mean of precision and recall\n",
        "Instead we use the harmonic meane\n",
        "\n",
        "There is a generalization of F-Measure called Fβ-Measure\n",
        "that balances between precision and recall\n",
        "Either\n",
        "P ≤ FM ≤ R or R ≤ FM ≤ P\n",
        "If P=0 or R=0 then FM=0\n",
        "\n",
        "```\n",
        "\n",
        "* Improving Text Classification\n",
        "\n",
        "  *Stopwords* - High frequency words are often not useful They occur in all documents Most applications don’t need them We can manually remove them stopword removal)\n",
        "\n",
        "  *Document Frequency* - Document frequency is the proportion of documents that contain a word.(Number of documents containing w / Total number of documents)\n",
        "\n",
        "  *TF - IDF* - This is often combined into a single score TF-IDF = Term Frequency Inverse Document Frequency. Occurrences of w in current document\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sH8U7sO_iy93"
      }
    }
  ]
}