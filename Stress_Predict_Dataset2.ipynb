{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1U5Yc1N162c4TZkToIVQ0TKgNdapv0LTs",
      "authorship_tag": "ABX9TyNfg+geJzzouYCsOvHiefJV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RafaelNovais/MasterAI/blob/master/Stress_Predict_Dataset2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DlozoDyiNbf"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Rafael Novais de Melo\n",
        "2324-CT5170 Principles of Machine Learning\n",
        "23113607\n",
        "\n",
        "\n",
        "###Understanding the Dataset\n",
        "For solving the problem of predicting stress levels, \"Supervised Classification\" is the appropriate machine learning category.\n",
        "This choice is justified by the binary data. It allows us to make predictions using the training dataset and confirm their accuracy using the test dataset,\n",
        "providing a quantitative estimate of stress levels and a more fine-grained understanding of stress.\n",
        "\n",
        "\n",
        "\n",
        "###Data Exploration\n",
        "Analyzing the distribution of data between training and testing sets is crucial. To determine whether the dataset is imbalanced, we can look at the proportion of each class.\n",
        "Imbalanced datasets typically have one class significantly outnumbering the other, but in this case,\n",
        "it doesn't qualify as imbalanced as the proportion falls within the range of low to moderate imbalances (less than 20%).\n",
        "Data:\n",
        "Class 0: 67.28%\n",
        "Class 1: 32.72%\n",
        "Test:\n",
        "Class 0: 67.18%\n",
        "Class 1: 32.82%\n",
        "Train:\n",
        "Class 0: 67.32%\n",
        "Class 1: 32.68%\n",
        "\n",
        "\n",
        "\n",
        "###Choosing an ML Package\n",
        "To address the various tasks associated with machine learning and classification, we rely on different tools:\n",
        "Pandas: It is used for reading and manipulating data in various formats, facilitating data conversion, cleaning, and organization.\n",
        "Scikit-Learn: This library offers a wide range of classification algorithms, including Decision Trees, Random Forests, Support Vector Machines (SVM), and Linear Regression,\n",
        "making it a valuable choice.\n",
        "Matplotlib and Seaborn: These are used for data visualization and scientific plotting.\n",
        "NumPy: It helps organize the data and perform mathematical operations.\n",
        "\n",
        "\n",
        "\n",
        "###Data Pre-processing\n",
        "Before applying machine learning algorithms, data pre-processing is essential. This involves removing empty rows, structuring the data in a dataframe,\n",
        "and establishing a baseline for accuracy comparison. The baseline serves as a reference point, assuming all predictions are 1 (random chance).\n",
        "\n",
        "\n",
        "\n",
        "###Algorithm Selection and Application\n",
        "The base line accuracy = 0.3283347657291883\n",
        "\n",
        "DecisionTreeClassifier: Initially, we set max_depth=5 to create a manageable visual representation. However, for improved precision and accuracy,\n",
        "it's better to remove the max_depth constraint. We use the confusion matrix to assess correct and incorrect predictions, verify recall,\n",
        "and calculate accuracy to compare with the baseline.\n",
        "accuracy = 0.7107844589989035\n",
        "precision = 0.559023430513325\n",
        "recall =0.5642205975268526\n",
        "confusion_matrix\n",
        "[17733  4931]\n",
        "[ 4828  6251]\n",
        "\n",
        "Random Forest: Employed with 200 decision trees, as it outperformed single decision trees in precision.\n",
        "accuracy = 0.7431170909521975\n",
        "precision = 0.6317918443205423\n",
        "recall = 0.5216174745013088\n",
        "confusion_matrix\n",
        "[19296  3368]\n",
        "[ 5300  5779]\n",
        "\n",
        "SVM: Unfortunately, SVM resulted in low accuracy and was not deemed trustworthy for this task.\n",
        "accuracy = 0.6716652342708117\n",
        "precision = 0\n",
        "recall = 0\n",
        "confusion_matrix\n",
        "[22664     0]\n",
        "[11079     0]\n",
        "\n",
        "\n",
        "AdaBoostClassifier and K-Nearest Neighbors: Produced similar results, but with less precision compared to Random Forest.\n",
        "AdaBoostClassifier\n",
        "accuracy = 0.6812968615712889\n",
        "precision = 0.5979505726341169\n",
        "recall = 0.08953876703673616\n",
        "confusion_matrix\n",
        "[21997   667]\n",
        "[10087   992]\n",
        "\n",
        "K-Nearest Neighbors\n",
        "accuracy = 0.7055981981448004\n",
        "precision = 0.6212666807879687\n",
        "recall = 0.26473508439389837\n",
        "confusion_matrix\n",
        "[20876  1788]\n",
        "[ 8146  2933]\n",
        "\n",
        "\n",
        "\n",
        "###Model Evaluation\n",
        "Given the health context of the problem, precision is considered the best metric. After applying various classification algorithms and calculating precision metrics,\n",
        "Random Forest emerged as the algorithm with the highest precision:\n",
        "Random Forest Precision = 0.6318\n",
        "\n",
        "\n",
        "\n",
        "###Comparative Analysis\n",
        "Both models provide different results, with Random Forest achieving the highest precision. The strengths and weaknesses of each algorithm's performance are reflected upon,\n",
        "taking into account factors like interpretability, accuracy, and the potential for overfitting or underfitting.\n",
        "The ability of the models to effectively predict stress levels is assessed in line with the assignment's objectives.\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from pandas.core.arrays.datetimelike import mode\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.tree import export_graphviz\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import graphviz\n",
        "from scipy.stats import ttest_rel\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "\n",
        "!pip install graphviz\n",
        "!apt-get install graphviz\n",
        "\n",
        "\n",
        "\n",
        "###Data Pre-processing\n",
        "#data = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Data.csv\")\n",
        "#data = pd.DataFrame(data)\n",
        "#data = data.dropna()\n",
        "#dataTest = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/test.csv\")\n",
        "#dataTest = pd.DataFrame(dataTest)\n",
        "#dataTest = dataTest.dropna()\n",
        "\n",
        "#x_data = data[[\"HR\",\"respr\"]]\n",
        "#y_data = data[\"Label\"]\n",
        "#x_test = dataTest[[\"HR\",\"respr\"]]\n",
        "#y_test = dataTest[\"Label\"]\n",
        "#x_train = dataTrain[[\"HR\",\"respr\"]]\n",
        "#y_train = dataTrain[\"Label\"]\n",
        "\n",
        "\n",
        "#Import the CSV Data and Cleaned\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/train.csv\")\n",
        "data = pd.DataFrame(data)\n",
        "data = data.dropna()\n",
        "\n",
        "#Split the Full data in Train and Test\n",
        "x_train, x_test, y_train, y_test = train_test_split( data[[\"HR\",\"respr\"]] , data[\"Label\"], test_size=0.20, random_state=10, stratify= data[\"Label\"] )\n",
        "#Check the split\n",
        "print(data.shape)\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "\n",
        "\n",
        "###Data Exploration\n",
        "#propor_data = y_data.value_counts()/len(x_data)\n",
        "propor_test = y_test.value_counts()/len(x_test)\n",
        "propor_train = y_train.value_counts()/len(x_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYe0QuMUKr-z",
        "outputId": "b34eeab7-28e1-403e-a524-ae984fa974e3"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (0.20.1)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "graphviz is already the newest version (2.42.2-6).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 19 not upgraded.\n",
            "(78729, 5)\n",
            "(62983, 2)\n",
            "(15746, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iEV5l1FjPaJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wB-eHlKcwcQy",
        "outputId": "a1cdbbf5-2637-41a9-cc7b-0783da879bc0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###BaseLine\n",
        "base_line = np.ones(len(y_test))\n",
        "base_line_accuracy = accuracy_score(y_test,base_line)\n",
        "base_line_precission = precision_score(y_test,base_line)\n",
        "print(base_line_precission)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdtXHd81-2HG",
        "outputId": "71269f2f-56b0-42a2-fffc-59f1e3e0f65e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.32687666709005464\n",
            "0.32687666709005464\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "###Algorithm Selection and Application\n",
        "##Decision Tree\n",
        "model_tree = DecisionTreeClassifier( random_state=0, max_depth=5)\n",
        "model_tree.fit(x_train,y_train)\n",
        "predict_tree = model_tree.predict(x_test)\n",
        "\n",
        "###Model Evaluation\n",
        "accuracy_tree = accuracy_score(y_test,predict_tree)\n",
        "precision_tree = precision_score(y_test,predict_tree)\n",
        "confusion_matrix_tree = confusion_matrix(y_test,predict_tree)\n",
        "recall_tree = recall_score(y_test,predict_tree)\n",
        "f1_tree = f1_score(y_test,predict_tree)\n",
        "print(accuracy_tree, precision_tree,recall_tree ,confusion_matrix_tree, f1_tree)\n",
        "#Print Graphic\n",
        "dot_data = export_graphviz(model_tree, feature_names=x_train.columns, class_names=[\"no\", \"yes\"] ,filled=True, rounded=True)\n",
        "graphviz.Source(dot_data)\n"
      ],
      "metadata": {
        "id": "bao3az--UOC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###Random Forest\n",
        "model_random = RandomForestClassifier(n_estimators=200 ,random_state=0)\n",
        "model_random.fit(x_train,y_train)\n",
        "predict_random = model_random.predict(x_test)\n",
        "\n",
        "###Model Evaluation\n",
        "accuracy_random = accuracy_score(y_test,predict_random)\n",
        "precision_random = precision_score(y_test,predict_random)\n",
        "confusion_matrix_random = confusion_matrix(y_test,predict_random)\n",
        "recall_random = recall_score(y_test,predict_random)\n",
        "f1_random = f1_score(y_test,predict_random)\n",
        "print(accuracy_random, precision_random,recall_random ,confusion_matrix_random,f1_random)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PqK94K5C08C",
        "outputId": "bbe72a94-df7f-4887-daa5-0744b28921c4"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7342817223421821 0.6144521036367958 0.5022343112492714 [[8977 1622]\n",
            " [2562 2585]] 0.5527047252512294\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Support Vector Machine (SVM)\n",
        "model_svm = LinearSVC(random_state=0)\n",
        "model_svm.fit(x_train,y_train)\n",
        "predict_svm = model_svm.predict(x_test)\n",
        "\n",
        "###Model Evaluation\n",
        "accuracy_svm = accuracy_score(y_test,predict_svm)\n",
        "precision_svm = precision_score(y_test,predict_svm)\n",
        "confusion_matrix_svm = confusion_matrix(y_test,predict_svm)\n",
        "recall_svm = recall_score(y_test,predict_svm)\n",
        "f1_svm = f1_score(y_test,predict_svm)\n",
        "print(accuracy_svm, precision_svm,recall_svm ,confusion_matrix_svm, f1_svm)\n",
        "\n",
        "##Print Graphic\n",
        "##sns.countplot(x=\"HR\",y=\"respr\",col=\"Label\", hue=\"Label\",data=data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ViTVbojcdm3",
        "outputId": "65cd4dff-5b70-4ee2-966a-9e2f2f9842d2"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6731233329099454 0.0 0.0 [[10599     0]\n",
            " [ 5147     0]] 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##AdaBoost\n",
        "model_boost = AdaBoostClassifier(n_estimators=200 ,random_state=0)\n",
        "model_boost.fit(x_train,y_train)\n",
        "predict_boost = model_boost.predict(x_test)\n",
        "\n",
        "###Model Evaluation\n",
        "accuracy_boost = accuracy_score(y_test,predict_boost)\n",
        "precision_boost = precision_score(y_test,predict_boost)\n",
        "confusion_matrix_boost = confusion_matrix(y_test,predict_boost)\n",
        "recall_boost = recall_score(y_test,predict_boost)\n",
        "f1_boost =  f1_score(y_test,predict_boost)\n",
        "print(accuracy_boost, precision_boost,recall_boost ,confusion_matrix_boost,f1_boost)"
      ],
      "metadata": {
        "id": "80lBku7mXRKV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6642de56-e4ef-42bf-b2d7-843da62f382a"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6782039883144926 0.5586510263929618 0.07402370312803574 [[10298   301]\n",
            " [ 4766   381]] 0.1307256819351518\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##K-Nearest Neighbors\n",
        "model_knn = KNeighborsClassifier(metric='euclidean', n_neighbors=100)\n",
        "model_knn.fit(x_train,y_train)\n",
        "predict_knn = model_knn.predict(x_test)\n",
        "\n",
        "###Model Evaluation\n",
        "accuracy_knn = accuracy_score(y_test,predict_knn)\n",
        "precision_knn = precision_score(y_test,predict_knn)\n",
        "confusion_matrix_knn = confusion_matrix(y_test,predict_knn)\n",
        "recall_knn = recall_score(y_test,predict_knn)\n",
        "f1_knn = f1_score(y_test,predict_knn)\n",
        "print(accuracy_knn, precision_knn,recall_knn ,confusion_matrix_knn,f1_knn)\n"
      ],
      "metadata": {
        "id": "lbE59aE_0N2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "526fc89e-5cba-462f-8628-865dae795ef6"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6992252000508066 0.5947441217150761 0.2506314357878376 [[9720  879]\n",
            " [3857 1290]] 0.35265172225259706\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Compare the precison\n",
        "algorithms = [\"Base Liine\",\"Decision Tree\", \"Random Forest\", \"K-Nearest Neighbors\", \"AdaBoost\"]\n",
        "precision_scores = [base_line_precission, precision_tree, precision_random, precision_knn, precision_boost]\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(algorithms, precision_scores, color=['blue', 'green'])\n",
        "plt.xlabel(\"Classification Algorithm\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision Comparison of Classification Algorithms\")\n",
        "plt.ylim(0, 1)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "for i, score in enumerate(precision_scores):\n",
        "    plt.text(i, score + 0.02, f\"{score:.2f}\", ha='center', va='bottom', fontsize=12)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "_ZxZsHE3MA89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#T-Test Compare Base line Precision\n",
        "precision_scores = [precision_tree, precision_random, precision_knn, precision_boost]\n",
        "print(precision_scores)\n",
        "significant_results = []\n",
        "p=0.05 # Significantly 95%\n",
        "for precision in precision_scores:\n",
        "    p_value = ttest_rel([base_line_precission], [precision]).pvalue\n",
        "    if p_value < p:\n",
        "        result = \"Significantly\"\n",
        "    else:\n",
        "        result = \"Not Significantly\"\n",
        "    significant_results.append(result)\n",
        "classifier_labels = [\"Decision Tree\", \"Random Forest\", \"K-NN\", \"Boosting\"]\n",
        "for i, classifier in enumerate(classifier_labels):\n",
        "    print(f\"{classifier} is {significant_results[i]} compared to the baseline.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNCtQoY3XrwA",
        "outputId": "df6a51d7-33c3-4110-feb6-2416682b8d69"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.5549389567147613, 0.6144521036367958, 0.5947441217150761, 0.5586510263929618]\n",
            "Decision Tree is Not Significantly Better compared to the baseline.\n",
            "Random Forest is Not Significantly Better compared to the baseline.\n",
            "K-NN is Not Significantly Better compared to the baseline.\n",
            "Boosting is Not Significantly Better compared to the baseline.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#KFold with the Classifier was done before\n",
        "#Suggestion of evolution : create a function for each classifier, and just call the function and compare the score per classifier\n",
        "\n",
        "X = data[[\"HR\", \"respr\"]]\n",
        "y = data[\"Label\"]\n",
        "\n",
        "classifiers = [DecisionTreeClassifier(), RandomForestClassifier(), LinearSVC(), AdaBoostClassifier(), KNeighborsClassifier()]\n",
        "classifier_names = [\"Decision Tree\", \"Random Forest\", \"LinearSVC\", \"AdaBoost\", \"K-NN\"]\n",
        "\n",
        "for classifier, classifier_name in zip(classifiers, classifier_names):\n",
        "    accuracy_scores = []\n",
        "    precision_scores = []\n",
        "    recall_scores = []\n",
        "    f1_scores = []\n",
        "\n",
        "    kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "    for train_index, test_index in kf.split(X):\n",
        "        X_train_fold, X_test_fold = X.iloc[train_index], X.iloc[test_index]\n",
        "        y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "        classifier.fit(X_train_fold, y_train_fold)\n",
        "        y_pred = classifier.predict(X_val_fold)\n",
        "\n",
        "        accuracy = accuracy_score(y_val_fold, y_pred)\n",
        "        precision = precision_score(y_val_fold, y_pred)\n",
        "        recall = recall_score(y_val_fold, y_pred)\n",
        "        f1 = f1_score(y_val_fold, y_pred)\n",
        "\n",
        "        accuracy_scores.append(accuracy)\n",
        "        precision_scores.append(precision)\n",
        "        recall_scores.append(recall)\n",
        "        f1_scores.append(f1)\n",
        "\n",
        "    mean_accuracy = accuracy_score(y_val_fold, y_pred)\n",
        "    mean_precision = precision_score(y_val_fold, y_pred)\n",
        "    mean_recall = recall_score(y_val_fold, y_pred)\n",
        "    mean_f1 = f1_score(y_val_fold, y_pred)\n",
        "\n",
        "    algorithms = [\"mean_accuracy\", \"mean_precision\", \"mean_recall\", \"mean_f1\"]\n",
        "    precision_scores = [ mean_accuracy, mean_precision, mean_recall, mean_f1]\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(algorithms, precision_scores, color=['blue', 'green'])\n",
        "    plt.xlabel(\"Classification score\")\n",
        "    plt.ylabel(\"score\")\n",
        "    plt.title((f\"Classifier: {classifier_name}\"))\n",
        "    plt.ylim(0, 1)\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    for i, score in enumerate(precision_scores):\n",
        "        plt.text(i, score + 0.02, f\"{score:.2f}\", ha='center', va='bottom', fontsize=12)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QrC-yWkygi0Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}