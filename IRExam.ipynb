{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOawoh+dzq4axYxcYhIBqnR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RafaelNovais/MasterAI/blob/master/IRExam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Weightinh Schemes\n",
        "* Describe, in your own words, with reference to any well-known term weighting scheme, the main constituents of a good weighting scheme.\n",
        "\n",
        "A good weighting scheme in information retrieval, such as TF-IDF (Term Frequency-Inverse Document Frequency) or related schemes, is designed to assign a weight to terms in a document based on their importance relative to the document and the entire collection (corpus). The main constituents of a good weighting scheme include:\n",
        "\n",
        "1. Term Frequency (TF)\n",
        "Purpose: Measures how often a term occurs in a document.\n",
        "Reasoning: Terms that appear more frequently in a document are likely more relevant to its content.\n",
        "Challenges: Raw frequency alone can overemphasize commonly occurring terms in longer documents, requiring normalization (e.g., by document length).\n",
        "\n",
        "**qnt frenquency/total qnt - in the document**\n",
        "\n",
        "2. Inverse Document Frequency (IDF)\n",
        "Purpose: Measures the rarity of a term across the entire corpus.\n",
        "Reasoning: Terms that occur in many documents (e.g., \"the\", \"is\") are less useful for distinguishing one document from another.\n",
        "Formula: Typically expressed as IDF\n",
        "\n",
        "**Qnt total Document / Qnt total document with term - in all document**\n",
        "\n",
        "Challenges: Rare terms are not always more relevant, and logarithmic scaling is often used to moderate IDF values.\n",
        "\n",
        "2. Normalization\n",
        "Purpose: Ensures comparability between documents of varying lengths and term distributions.\n",
        "Methods: Can include cosine normalization or length-based scaling to avoid bias toward longer documents.\n",
        "Benefit: Improves ranking fairness and retrieval accuracy.\n",
        "\n",
        "--------------------------------------------------\n",
        "#2. Evaluate\n",
        "* Precision and recall are often used to measure the quality of an IR system. Explain what is meant by these terms. Suggest any alternative approach that you might consider to measure the quality of the IR system.\n",
        "\n",
        "Precision and Recall are key metrics in Information Retrieval (IR) used to evaluate the effectiveness of a system's ability to retrieve relevant documents for a query.\n",
        "\n",
        "1. Precision\n",
        "\n",
        "Definition: The proportion of retrieved documents that are relevant.\n",
        "\n",
        "Example: If an IR system retrieves 10 documents, and 7 of them are relevant, the precision is 70%.\n",
        "\n",
        "Significance: Indicates the system‚Äôs ability to avoid retrieving irrelevant documents.\n",
        "\n",
        "2. Recall\n",
        "\n",
        "Definition: The proportion of all relevant documents in the corpus that are retrieved.\n",
        "\n",
        "Example: If there are 50 relevant documents in the corpus, and the system retrieves 25 of them, the recall is 50%.\n",
        "\n",
        "Significance: Indicates the system‚Äôs ability to retrieve all relevant documents.\n",
        "\n",
        "\n",
        "**Alternative Metrics to Measure Quality**\n",
        "\n",
        " 1. F1-Score -\n",
        "\n",
        " Definition: Harmonic mean of precision and recall, providing a single measure of balance.\n",
        "\n",
        "  Use Case: Useful when a balance between precision and recall is desired\n",
        "\n",
        "2. Mean Average Precision (MAP)\n",
        "\n",
        "  Definition: Computes the average precision across all queries in a dataset.\n",
        "\n",
        "  Advantage: Takes the ranking of retrieved documents into account, rewarding systems that place relevant documents higher.\n",
        "\n",
        "\n",
        "----------------------------------------------------------------\n",
        "#3. Preprocessing\n",
        "* Outline a suitable approach to indexing a document collection to allow efficient handling of queries in a system adopting a vector space framework.\n",
        "\n",
        "1. Preprocessing the Document Collection\n",
        "  * Tokenization: Break text into individual terms (e.g., words or phrases).\n",
        "  * Stop Word Removal: Exclude common, non-informative words (e.g., \"the\", \"is\").\n",
        "  * Stemming/Lemmatization: Reduce words to their root forms (e.g., \"running\" ‚Üí \"run\").\n",
        "  * Lowercasing: Normalize terms to lowercase for case-insensitive matching.\n",
        "\n",
        "\n",
        "2. Feature Extraction\n",
        "  * Term Frequency (TF): Count occurrences of each term in a document.\n",
        "  * Inverse Document Frequency (IDF): Compute the rarity of each term across the corpus.\n",
        "  * TF-IDF Weighting: Calculate weighted term importance for each document\n",
        "\n",
        "------------------------------------------------------------------\n",
        "#1. Feedback\n",
        "* Discuss an approach where user feedback (users identifying whether returned documents are relevant or not) can be used to extend or improve the query. Outline any issues with this approach.\n",
        "\n",
        "  User feedback about the relevance of returned documents can be utilized to refine or extend the original query, improving retrieval performance. This approach, known as relevance feedback, is commonly used in systems adopting models like the vector space model or probabilistic retrieval.\n",
        "\n",
        "  1. Basic Idea\n",
        "  * Input: Users mark returned documents as \"relevant\" or \"non-relevant\" after an initial query.\n",
        "  * Process: The system adjusts the query based on this feedback by incorporating terms from relevant documents and de-emphasizing terms from non-relevant ones.\n",
        "  * Output: A refined query that retrieves more relevant documents in subsequent searches.\n",
        "\n",
        "  Relevance feedback is a powerful approach for improving query effectiveness in IR systems, leveraging user interactions to dynamically refine results. However, the approach requires careful handling of user effort, noise, and computational complexity to ensure practicality and scalability. Proper balancing of automation and user input can maximize its benefits.\n",
        "----------------------------------------------------------------------\n",
        "#2. Implicit Feedback\n",
        "* Predicting which query q, q' (modified through relevance feedback), is likely to perform better requires analyzing various aspects of their composition, context, and anticipated effectiveness. Below are strategies for making such predictions:\n",
        "\n",
        "**Query Quality Metrics**\n",
        "\n",
        "1. Query Specificit\n",
        "* Definition: Measures how focused or narrow a query is in identifying relevant documents.\n",
        "* Prediction: A more specific query (e.g., with unique, high-discrimination terms) is likely to perform better, as it reduces ambiguity.\n",
        "* Example: If q includes high-IDF terms from relevant documents, it may outperform q.\n",
        "2. Query Generality\n",
        "* Definition: Measures the breadth of terms included in a query.\n",
        "* Prediction: If the information need is broad,might perform better if it includes diverse terms, improving recall.\n",
        "\n",
        "To predict which query is likely to perform better, analyze metrics like specificity, precision, recall, and feedback utilization, while accounting for the characteristics of the document corpus and user needs. Empirical testing using relevance metrics (e.g., precision, recall) or predictive models can provide quantitative insights into performance differences between q and q'.\n",
        "\n",
        "FOR BETTER METRIC PERFORMACE\n",
        "\n",
        "#2. Pseudo FeedBack\n",
        "\n",
        "To provide a diverse set of candidate terms for users to augment their queries, a pseudo-feedback mechanism can analyze the initial query results and extract terms that represent different facets of the information need. The goal is to present terms that enhance both precision and recall while covering a range of related topics or contexts. Below is a suggested approach:\n",
        "\n",
        "1. Initial Retrieval\n",
        "  * Retrieve Documents: Execute the initial query and retrieve the top ùëÅ, N ranked documents (e.g., top 10-50).\n",
        "  * Extract Terms: Identify candidate terms from these documents. Candidate terms could include:\n",
        "    * High-frequency words.\n",
        "    * Proper nouns, named entities (e.g., person names, locations, organizations).\n",
        "    * Key phrases (using phrase detection algorithms).\n",
        "\n",
        "2. Scoring and Filtering Terms\n",
        "Assign scores to each extracted term to measure its importance and relevance:\n",
        "\n",
        "  * TF-IDF Weighting:\n",
        "  Prioritize terms that occur frequently in the top ùëÅ N documents but rarely in the entire corpus.\n",
        "\n",
        "\n",
        "3. Clustering for Diversity\n",
        "  * Cluster candidate terms into distinct groups to represent different facets of the topic:\n",
        "  * Algorithm: Use clustering techniques like **K-Means**, DBSCAN, or hierarchical clustering on term vectors (e.g., embeddings).\n",
        "  * Output: Each cluster represents a unique subtopic or context related to the query.\n",
        "\n",
        "\n",
        "This approach ensures a diverse and informative set of candidate terms for query augmentation. By using clustering and ranking mechanisms, the system captures various aspects of the query topic, enabling users to refine their search effectively. Integrating interactive features empowers users to customize the query further, balancing automation with user control.\n",
        "----------------------------------------------------------\n",
        "#3. Query Difficulty Prediction\n",
        "* Given a submitted query, we can process the query in many ways, e.g. query expansion. These additional techniques may be more beneficial for difficult queries. Suggest a suitable means to identify a difficult query.\n",
        "\n",
        "Identifying a difficult query is crucial for deciding when to apply additional techniques like query expansion, relevance feedback, or other refinements. A suitable approach would involve analyzing multiple aspects of the query and its initial retrieval performance, as detailed below:\n",
        "1.  Query Length\n",
        "2.  Query Specificity\n",
        "3.  Term Ambiguity\n",
        "4.\n",
        "\n",
        "\n",
        "A comprehensive approach to identifying difficult queries involves analyzing query structure, retrieval performance, term semantics, and user interaction data. Combining these metrics into a hybrid model ensures robust predictions, allowing the system to selectively apply advanced techniques like query expansion or clustering to improve retrieval effectiveness.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#1. Collab Filtering\n",
        "#2. Similarity\n",
        "#3. Clustering\n",
        "\n",
        "#1. Learning MIR\n",
        "#2. Format approches\n",
        "#3. Visualisation"
      ],
      "metadata": {
        "id": "-pEBzwUjeyw6"
      }
    }
  ]
}