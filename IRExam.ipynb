{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN5Az3XJY0288CSF1GNrUT+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RafaelNovais/MasterAI/blob/master/IRExam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Weightinh Schemes\n",
        "* Describe, in your own words, with reference to any well-known term weighting scheme, the main constituents of a good weighting scheme.\n",
        "\n",
        "A good weighting scheme in information retrieval, such as TF-IDF (Term Frequency-Inverse Document Frequency) or related schemes, is designed to assign a weight to terms in a document based on their importance relative to the document and the entire collection (corpus). The main constituents of a good weighting scheme include:\n",
        "\n",
        "1. Term Frequency (TF)\n",
        "Purpose: Measures how often a term occurs in a document.\n",
        "Reasoning: Terms that appear more frequently in a document are likely more relevant to its content.\n",
        "Challenges: Raw frequency alone can overemphasize commonly occurring terms in longer documents, requiring normalization (e.g., by document length).\n",
        "\n",
        "**qnt frenquency/total qnt - in the document**\n",
        "\n",
        "2. Inverse Document Frequency (IDF)\n",
        "Purpose: Measures the rarity of a term across the entire corpus.\n",
        "Reasoning: Terms that occur in many documents (e.g., \"the\", \"is\") are less useful for distinguishing one document from another.\n",
        "Formula: Typically expressed as IDF\n",
        "\n",
        "**Qnt total Document / Qnt total document with term - in all document**\n",
        "\n",
        "Challenges: Rare terms are not always more relevant, and logarithmic scaling is often used to moderate IDF values.\n",
        "\n",
        "2. Normalization\n",
        "Purpose: Ensures comparability between documents of varying lengths and term distributions.\n",
        "Methods: Can include cosine normalization or length-based scaling to avoid bias toward longer documents.\n",
        "Benefit: Improves ranking fairness and retrieval accuracy.\n",
        "\n",
        "--------------------------------------------------\n",
        "#2. Evaluate\n",
        "* Precision and recall are often used to measure the quality of an IR system. Explain what is meant by these terms. Suggest any alternative approach that you might consider to measure the quality of the IR system.\n",
        "\n",
        "Precision and Recall are key metrics in Information Retrieval (IR) used to evaluate the effectiveness of a system's ability to retrieve relevant documents for a query.\n",
        "\n",
        "1. Precision\n",
        "\n",
        "Definition: The proportion of retrieved documents that are relevant.\n",
        "\n",
        "Example: If an IR system retrieves 10 documents, and 7 of them are relevant, the precision is 70%.\n",
        "\n",
        "Significance: Indicates the system’s ability to avoid retrieving irrelevant documents.\n",
        "\n",
        "2. Recall\n",
        "\n",
        "Definition: The proportion of all relevant documents in the corpus that are retrieved.\n",
        "\n",
        "Example: If there are 50 relevant documents in the corpus, and the system retrieves 25 of them, the recall is 50%.\n",
        "\n",
        "Significance: Indicates the system’s ability to retrieve all relevant documents.\n",
        "\n",
        "\n",
        "**Alternative Metrics to Measure Quality**\n",
        "\n",
        " 1. F1-Score -\n",
        "\n",
        " Definition: Harmonic mean of precision and recall, providing a single measure of balance.\n",
        "\n",
        "  Use Case: Useful when a balance between precision and recall is desired\n",
        "\n",
        "2. Mean Average Precision (MAP)\n",
        "\n",
        "  Definition: Computes the average precision across all queries in a dataset.\n",
        "\n",
        "  Advantage: Takes the ranking of retrieved documents into account, rewarding systems that place relevant documents higher.\n",
        "\n",
        "\n",
        "----------------------------------------------------------------\n",
        "#3. Preprocessing\n",
        "* Outline a suitable approach to indexing a document collection to allow efficient handling of queries in a system adopting a vector space framework.\n",
        "\n",
        "1. Preprocessing the Document Collection\n",
        "  * Tokenization: Break text into individual terms (e.g., words or phrases).\n",
        "  * Stop Word Removal: Exclude common, non-informative words (e.g., \"the\", \"is\").\n",
        "  * Stemming/Lemmatization: Reduce words to their root forms (e.g., \"running\" → \"run\").\n",
        "  * Lowercasing: Normalize terms to lowercase for case-insensitive matching.\n",
        "\n",
        "\n",
        "2. Feature Extraction\n",
        "  * Term Frequency (TF): Count occurrences of each term in a document.\n",
        "  * Inverse Document Frequency (IDF): Compute the rarity of each term across the corpus.\n",
        "  * TF-IDF Weighting: Calculate weighted term importance for each document\n",
        "\n",
        "------------------------------------------------------------------\n",
        "#1. Feedback\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#2. Implicit Feedback\n",
        "#3. Query Difficulty Prediction\n",
        "\n",
        "#1. Collab Filtering\n",
        "#2. Similarity\n",
        "#3. Clustering\n",
        "\n",
        "#1. Learning MIR\n",
        "#2. Format approches\n",
        "#3. Visualisation"
      ],
      "metadata": {
        "id": "-pEBzwUjeyw6"
      }
    }
  ]
}