{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RafaelNovais/MasterAI/blob/master/Assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rafael Novais de Melo -\n",
        "23113607**"
      ],
      "metadata": {
        "id": "XoDBgUxtdcpv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsK6oiuNTGfN"
      },
      "source": [
        "### Problem definition\n",
        "\n",
        "A company ABC have 1238 employees. ABC wants to buy a new insurance policy for its employees. Attach is a dataset (train-2.csv Download train-2.csv) about some of their previous employees where the company paid medical costs billed by a health insurance company XYZ. The total cost was 16410282.7 euros.\n",
        "\n",
        "ABC recently recruited about 100 additional employees. XYZ gave them a high price (i.e. 17755825 euros) for the same policy considering there will be additional 100 employees.\n",
        "\n",
        "You are working in ABC as a data analyst/research engineer. Your boss asked you to rapidly use the existing data that you have to see whether the given price is reasonable or not. Can you accurately predict the costs for the new employees based on their given information? Report the total price (17755825 euros) that is the previous total price (16410282.7) plus the new predicted price for the new employees with specific features given in the testing set (test-3.csv Download test-3.csv).\n",
        "\n",
        "Do the following:\n",
        "\n",
        "First, report what type of algorithm you need to use and why? [1 Mark]\n",
        "Use all the techniques of pre-processing, cleaning, and normalisation to prepare your data. Report each with justification why you want to use. [3 Marks]\n",
        "Analyse the data by visualising each feature or overall whole dataset with various graphs, bars, etc. to understand the data before applying a model. Report what you learned from visualising the data. Did you find any correlation or discrepancies in the data. [3 Marks]\n",
        "Use the k-fold cross validation approach to find the optimal model that you can apply on the testing data provided. Report the performance e.g loss and loss curve,  validation accuracy and its curve (if applicable) and show when and why you stopped e.g., did you reached convergence or not. [2.75 Marks]\n",
        "Report the final total price that you got. [0.25 Marks]\n",
        "This time you need to apply and present the results in the best way possible to your boss. Considering you have learned various ways.\n",
        "\n",
        "Upload your pdf of not more than 4 pages in the area designated for pdf upload.\n",
        "\n",
        "In addition, upload the code in the designated area for code upload. Plagiarism will be strictly penalised."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flWkipQVTGfT"
      },
      "outputs": [],
      "source": [
        "#Read and import file\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.spatial import ConvexHull\n",
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import seaborn as sns\n",
        "from scipy.spatial import ConvexHull\n",
        "import requests\n",
        "from io import StringIO\n",
        "\n",
        "\n",
        "#df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/train-2.csv')\n",
        "#url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
        "\n",
        "#Validation Data Prepared\n",
        "df = upload_file()\n",
        "selected_column = select_column(df)\n",
        "cleaned_column = clean_df(selected_column)\n",
        "\n",
        "\n",
        "#Function\n",
        "print('Convex Hull')\n",
        "convex_hull(cleaned_column)\n",
        "print('Isolation Forest')\n",
        "isolation_forest(cleaned_column)\n",
        "print('Bounding Box')\n",
        "bounding_box(cleaned_column)\n",
        "print('Outliers')\n",
        "outliers(cleaned_column)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Upload File from ChatGpt\n",
        "\n",
        "def upload_file(): #Upload/Read csv file\n",
        "    file_path = input(\"Please enter the file path or URL to upload: \")\n",
        "    df = pd.read_csv(file_path)\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "QkMhm3lnUtj9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Clean the dataframe\n",
        "def clean_df(data):\n",
        "  cleaned_data = data.dropna()\n",
        "  return cleaned_data\n",
        ""
      ],
      "metadata": {
        "id": "D0q8vRIot6GS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_column(data): # Function to select 2 columns in the data to identify the periphery\n",
        "\n",
        "        print(\"Columns in the DataFrame:\") #Show all columns names to select 2\n",
        "        for column in data.columns:\n",
        "            print(column)\n",
        "\n",
        "\n",
        "        selected_columns = [] #Select 2 columns from the dataset and inclued in a new dataframe\n",
        "        while len(selected_columns) < 2:\n",
        "            column_name = input(\"Enter the name of a column: \")\n",
        "            if column_name in data.columns:\n",
        "                selected_columns.append(column_name)\n",
        "            else:\n",
        "                print(\"Please enter a valid column name.\")\n",
        "\n",
        "\n",
        "        selected_df = data[selected_columns]\n",
        "\n",
        "        return selected_df"
      ],
      "metadata": {
        "id": "urdAYrwcUyKg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ConvexHull\n",
        "def convex_hull(data):\n",
        "    xy = data.values\n",
        "    hull = ConvexHull(xy) # Convex Hull is a function where the next convex angle is found.\n",
        "    periphery_indices = hull.vertices #Set the index os all periphery in a variable\n",
        "    periphery_indices = periphery_indices.astype(int)\n",
        "    periphery_points = data.iloc[periphery_indices] #Set the all periphery points in the graphic in a variable\n",
        "    plt.scatter(data.iloc[:, 0], data.iloc[:, 1], s=50, c='b', label='Main') #Plot the all the points in the graphic\n",
        "    plt.scatter(periphery_points.iloc[:, 0], periphery_points.iloc[:, 1], s=50, c='r', label='Periphery') # Paint the periphery in red and save in graphic\n",
        "    plt.plot(data.iloc[periphery_indices, 0], data.iloc[periphery_indices, 1], 'k--', lw=2)#Conect all the periphery point with line and save in graphic\n",
        "    plt.legend()# Display the graphic\n",
        "    plt.title('Convex Hull')\n",
        "    plt.xlabel(data.columns[0])\n",
        "    plt.ylabel(data.columns[1])\n",
        "    plt.show()# Display the graphic\n",
        "\n"
      ],
      "metadata": {
        "id": "pfIOIBICbS5X"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convex Hull:\n",
        "Is good to see the data distribution and the detected outliers.\n",
        "But is not good with Mult Dimension"
      ],
      "metadata": {
        "id": "CjSIHEKIVEP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#IsolationForest\n",
        "def isolation_forest(data):\n",
        "\n",
        "  model = IsolationForest(contamination=0.05)  # Adjust contamination based on your dataset\n",
        "  model.fit(data) # Fit the model to the data\n",
        "  outliers = model.predict(data) # Predict anomalies (outliers)\n",
        "  anomaly_points = data[outliers == -1] # Extract the anomaly points\n",
        "  plt.scatter(data.iloc[:, 0], data.iloc[:, 1], s=50, c='b', label='Normal Data')\n",
        "  plt.scatter(anomaly_points.iloc[:, 0], anomaly_points.iloc[:, 1], s=50, c='r', label='Anomaly Points')\n",
        "  plt.legend()\n",
        "  plt.xlabel(data.columns[0])\n",
        "  plt.ylabel(data.columns[1])\n",
        "  plt.title('Isolation Forest')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "7IZEtbusJB0T"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Isolation Forest: s a fast and efficient algorithm for identifying outliers. It's especially useful for large datasets / Mult Dimension. The performace is not good"
      ],
      "metadata": {
        "id": "zND8fcB6Vtiq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Outliers\n",
        "def outliers(data):\n",
        "  plt.figure(figsize=(20, 10))\n",
        "  sns.boxplot(x=data[data.columns[0]], y=data[data.columns[1]], color='lightblue', width=0.6)\n",
        "  sns.stripplot(x=data[data.columns[0]], y=data[data.columns[1]], color='red', size=4, jitter=0.2)\n",
        "  plt.xlabel(data.columns[0])\n",
        "  plt.ylabel(data.columns[1])\n",
        "  plt.title(f\"Outliers\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "FT5iW2HqjZve"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outliers: is good to analise the extremes for each feature and the performace is good to robust data and identify if have any anomaly, but could not be so efective and show some false positives"
      ],
      "metadata": {
        "id": "hT2XA7nPYfYu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bounding Box using ChatGPT\n",
        ">\n",
        "Rafael : can you do an exemple of bounding box with python.\n",
        ">\n",
        "ChatGPT: Generated\n",
        "```\n",
        "# Extract the feature columns\n",
        "# Calculate the minimum and maximum values for each feature\n",
        "# Define the bounding box\n",
        "```\n",
        "Rafael: how can i do a plot with the anwser in Bounding Box:\n",
        "```\n",
        "# Filter points within the bounding box\n",
        "# Create a scatter plot for all data points\n",
        "# Create a rectangle for the bounding box\n",
        "# Plot the\n",
        "# Highlight points within the bounding box\n",
        "```\n",
        "Rafael: ValueError: Can only compare identically-labeled Series objects\n",
        "---> 19     (X[\"temp_max\"] >= bounding_box[\"min_temp_max\n",
        ">\n",
        "ChatGPT: Fixed the Code, and return the atual code\n",
        "```\n",
        "This corrected code should allow you to create a plot with the bounding box and points within the box for your dataset. Make sure to replace \"dataset.csv\" with the path to your CSV file and adjust the column names as needed.\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LD8CE_FINK8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bounding Box from ChatGPT\n",
        "def bounding_box(data):\n",
        "    # Extract the feature columns\n",
        "    X = data\n",
        "\n",
        "    # Calculate the minimum and maximum values for each feature\n",
        "    min_vals = X.min()\n",
        "    max_vals = X.max()\n",
        "\n",
        "    # Define the bounding box using column names\n",
        "    bounding_box = {\n",
        "        f'min_{X.columns[0]}': min_vals[0],\n",
        "        f'max_{X.columns[0]}': max_vals[0],\n",
        "        f'min_{X.columns[1]}': min_vals[1],\n",
        "        f'max_{X.columns[1]}': max_vals[1]\n",
        "    }\n",
        "\n",
        "    # Filter points within the bounding box\n",
        "    points_within_box = X[\n",
        "        (X[X.columns[0]] >= bounding_box[f'min_{X.columns[0]}']) &\n",
        "        (X[X.columns[0]] <= bounding_box[f'max_{X.columns[0]}']) &\n",
        "        (X[X.columns[1]] >= bounding_box[f'min_{X.columns[1]}']) &\n",
        "        (X[X.columns[1]] <= bounding_box[f'max_{X.columns[1]}'])\n",
        "    ]\n",
        "\n",
        "    # Create a scatter plot for all data points\n",
        "    plt.scatter(X[X.columns[0]], X[X.columns[1]], c='b', label='Data Points')\n",
        "\n",
        "    # Create a rectangle for the bounding box\n",
        "    rectangle = plt.Rectangle(\n",
        "        (bounding_box[f'min_{X.columns[0]}'], bounding_box[f'min_{X.columns[1]}']),\n",
        "        bounding_box[f'max_{X.columns[0]}'] - bounding_box[f'min_{X.columns[0]}'],\n",
        "        bounding_box[f'max_{X.columns[1]}'] - bounding_box[f'min_{X.columns[1]}'],\n",
        "        color='r',\n",
        "        fill=False,\n",
        "        label='Bounding Box'\n",
        "    )\n",
        "\n",
        "    # Plot the rectangle\n",
        "    plt.gca().add_patch(rectangle)\n",
        "\n",
        "    # Highlight points within the bounding box\n",
        "    plt.scatter(points_within_box[X.columns[0]], points_within_box[X.columns[1]], c='g', label='Points in Bounding Box')\n",
        "\n",
        "    plt.xlabel(X.columns[0])\n",
        "    plt.ylabel(X.columns[1])\n",
        "    plt.legend()\n",
        "    plt.title(f\"Bounding Box and Data Points for {X.columns[0]} and {X.columns[1]}\")\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "Mzqfg0n67P8a"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bounding Box: is good to data exploration to find outliers in the data range but sensitive to the range of data values and may not capture outliers"
      ],
      "metadata": {
        "id": "SfsOd9i-Z-8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTqQuKZQLCF0",
        "outputId": "8c1ee437-1e0f-4c95-cf96-d3a62eef1118"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Periphery of a dataset is important for assessing a model's generalization, robustness, and its ability to handle unseen or extreme data points, including predictive modeling, anomaly detection, and fairness in machine learning."
      ],
      "metadata": {
        "id": "nhvmp8Wnipvy"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py39",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}