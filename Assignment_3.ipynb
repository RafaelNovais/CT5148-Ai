{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RafaelNovais/MasterAI/blob/master/Assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rafael Novais de Melo -\n",
        "23113607**"
      ],
      "metadata": {
        "id": "XoDBgUxtdcpv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsK6oiuNTGfN"
      },
      "source": [
        "### Problem definition\n",
        "\n",
        "A company ABC have 1238 employees. ABC wants to buy a new insurance policy for its employees. Attach is a dataset (train-2.csv Download train-2.csv) about some of their previous employees where the company paid medical costs billed by a health insurance company XYZ. The total cost was 16410282.7 euros.\n",
        "\n",
        "ABC recently recruited about 100 additional employees. XYZ gave them a high price (i.e. 17755825 euros) for the same policy considering there will be additional 100 employees.\n",
        "\n",
        "You are working in ABC as a data analyst/research engineer. Your boss asked you to rapidly use the existing data that you have to see whether the given price is reasonable or not. Can you accurately predict the costs for the new employees based on their given information? Report the total price (17755825 euros) that is the previous total price (16410282.7) plus the new predicted price for the new employees with specific features given in the testing set (test-3.csv Download test-3.csv).\n",
        "\n",
        "Do the following:\n",
        "\n",
        "First, report what type of algorithm you need to use and why? [1 Mark]\n",
        "Use all the techniques of pre-processing, cleaning, and normalisation to prepare your data. Report each with justification why you want to use. [3 Marks]\n",
        "Analyse the data by visualising each feature or overall whole dataset with various graphs, bars, etc. to understand the data before applying a model. Report what you learned from visualising the data. Did you find any correlation or discrepancies in the data. [3 Marks]\n",
        "Use the k-fold cross validation approach to find the optimal model that you can apply on the testing data provided. Report the performance e.g loss and loss curve,  validation accuracy and its curve (if applicable) and show when and why you stopped e.g., did you reached convergence or not. [2.75 Marks]\n",
        "Report the final total price that you got. [0.25 Marks]\n",
        "This time you need to apply and present the results in the best way possible to your boss. Considering you have learned various ways.\n",
        "\n",
        "Upload your pdf of not more than 4 pages in the area designated for pdf upload.\n",
        "\n",
        "In addition, upload the code in the designated area for code upload. Plagiarism will be strictly penalised."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "flWkipQVTGfT"
      },
      "outputs": [],
      "source": [
        "#import Liberies\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.spatial import ConvexHull\n",
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import seaborn as sns\n",
        "from scipy.spatial import ConvexHull\n",
        "import requests\n",
        "from io import StringIO\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Read csv file\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/train-2.csv')\n",
        "#df = upload_file()\n",
        "\n",
        "\n",
        "#Validation Data Prepared\n",
        "#df = upload_file()\n",
        "cleaned_df = clean_df(df)\n",
        "#test_df = upload_file()\n",
        "#cleaned_test_df = clean_df(test_df)\n",
        "\n",
        "#Select data and split in Train and Test\n",
        "x_train, x_test, y_train, y_test  = select_split_data(cleaned_df)\n",
        "\n",
        "#Encode the categorical variables\n",
        "categorical_cols = select_cat(x_train)\n",
        "x_train_encode = encode_cat(x_train,categorical_cols)\n",
        "x_test_encode = encode_cat(x_test,categorical_cols)\n",
        "\n",
        "#Classification\n",
        "model_random_forest = class_randomforest(x_train_encode,y_train)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Function\n",
        "#print('Convex Hull')\n",
        "#convex_hull(cleaned_column)\n",
        "#print('Isolation Forest')\n",
        "#isolation_forest(cleaned_column)\n",
        "#print('Bounding Box')\n",
        "#bounding_box(cleaned_column)\n",
        "#print('Outliers')\n",
        "#outliers(cleaned_column)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        },
        "id": "ay8ytioQWumd",
        "outputId": "aca28b13-885d-4316-810d-b1487cbbf2d8"
      },
      "execution_count": 95,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns in the DataFrame:\n",
            "age\n",
            "sex\n",
            "bmi\n",
            "children\n",
            "smoker\n",
            "region\n",
            "charges\n",
            "Enter the name of the class label column: charges\n",
            "Columns in the DataFrame:\n",
            "age\n",
            "sex\n",
            "bmi\n",
            "children\n",
            "smoker\n",
            "region\n",
            "charges\n",
            "Enter the names of categorical columns separated by ,: sex,smoker,region\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-95-62fd9b80f5fc>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#Classification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mmodel_random_forest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_randomforest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_encode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-68-97248fc9fb14>\u001b[0m in \u001b[0;36mclass_randomforest\u001b[0;34m(x_train, y_train)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclass_randomforest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel_random\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmodel_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_random\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_class_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_y_class_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mDOUBLE\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_validate_y_class_weight\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_y_class_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    747\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36mcheck_classification_targets\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;34m\"multilabel-sequences\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     ]:\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown label type: %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unknown label type: 'continuous'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape,x_train_encode.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuIVG2gL0J2X",
        "outputId": "0d054902-e626-44f7-e0f8-d24d56b6e269"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1176, 6) (1176, 11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Upload File\n",
        "\n",
        "def upload_file(): #Upload/Read csv file\n",
        "    file_path = input(\"Please enter the file path or URL to upload: \")\n",
        "    df = pd.read_csv(file_path)\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "QkMhm3lnUtj9"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Clean the dataframe\n",
        "def clean_df(data):\n",
        "  cleaned_data = data.dropna()\n",
        "  return cleaned_data\n"
      ],
      "metadata": {
        "id": "D0q8vRIot6GS"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Select features in X column and label column in Y and Split the Data into Train/Test\n",
        "def select_split_data(data):\n",
        "    print(\"Columns in the DataFrame:\")  # Show all column names to select 2\n",
        "    for column in data.columns:\n",
        "        print(column)\n",
        "\n",
        "    column_name = input(\"Enter the name of the class label column: \")\n",
        "    if column_name not in data.columns:\n",
        "        raise ValueError(\"Invalid column name. Please enter a valid column name.\")\n",
        "\n",
        "    x_data = data.drop(columns=column_name)  # Drop columns specified in column_name from x_data\n",
        "    y_data = data[column_name]\n",
        "\n",
        "\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.05, random_state=10)\n",
        "\n",
        "    return x_train, x_test, y_train, y_test\n"
      ],
      "metadata": {
        "id": "urdAYrwcUyKg"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the columns are categorical\n",
        "def select_cat(x_data):\n",
        "    print(\"Columns in the DataFrame:\")\n",
        "    for column in df.columns:\n",
        "        print(column)\n",
        "\n",
        "    categorical_cols_str = input(\"Enter the names of categorical columns separated by ,: \")\n",
        "    categorical_cols = categorical_cols_str.split(',')\n",
        "    return categorical_cols"
      ],
      "metadata": {
        "id": "TAD8O9P_MI4H"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encode categorical variables into numeric representations\n",
        "def encode_cat(x_data,categorical_cols):\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('cat', OneHotEncoder(), categorical_cols)\n",
        "        ],\n",
        "        remainder='passthrough'\n",
        "    )\n",
        "\n",
        "    x_encoded = preprocessor.fit_transform(x_data)\n",
        "\n",
        "    return x_encoded"
      ],
      "metadata": {
        "id": "f9248AtKIZVZ"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Random Forest Classification\n",
        "def class_randomforest(x_train,y_train):\n",
        "    model_random = RandomForestClassifier(n_estimators=200 ,random_state=0)\n",
        "    model_random.fit(x_train,y_train)\n",
        "\n",
        "    return model_random\n",
        "\n"
      ],
      "metadata": {
        "id": "2eXi8nPl_30W"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##K-Nearest Neighbors Classification\n",
        "def class_knn((x_train,y_train))\n",
        "    model_knn = KNeighborsClassifier(metric='euclidean', n_neighbors=100)\n",
        "    model_knn.fit(x_train,y_train)\n",
        "\n",
        "    return model_knn\n",
        "\n",
        "    predict_knn = model_knn.predict(x_test)"
      ],
      "metadata": {
        "id": "2w2B5FxLEMS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Evaluation compare\n",
        "\n",
        "\n",
        "accuracy_random = accuracy_score(y_test,predict_random)\n",
        "precision_random = precision_score(y_test,predict_random)\n",
        "confusion_matrix_random = confusion_matrix(y_test,predict_random)\n",
        "recall_random = recall_score(y_test,predict_random)\n",
        "print(accuracy_random, precision_random,recall_random ,confusion_matrix_random)\n"
      ],
      "metadata": {
        "id": "yxdsSJDIERDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ConvexHull\n",
        "def convex_hull(data):\n",
        "    xy = data.values\n",
        "    hull = ConvexHull(xy) # Convex Hull is a function where the next convex angle is found.\n",
        "    periphery_indices = hull.vertices #Set the index os all periphery in a variable\n",
        "    periphery_indices = periphery_indices.astype(int)\n",
        "    periphery_points = data.iloc[periphery_indices] #Set the all periphery points in the graphic in a variable\n",
        "    plt.scatter(data.iloc[:, 0], data.iloc[:, 1], s=50, c='b', label='Main') #Plot the all the points in the graphic\n",
        "    plt.scatter(periphery_points.iloc[:, 0], periphery_points.iloc[:, 1], s=50, c='r', label='Periphery') # Paint the periphery in red and save in graphic\n",
        "    plt.plot(data.iloc[periphery_indices, 0], data.iloc[periphery_indices, 1], 'k--', lw=2)#Conect all the periphery point with line and save in graphic\n",
        "    plt.legend()# Display the graphic\n",
        "    plt.title('Convex Hull')\n",
        "    plt.xlabel(data.columns[0])\n",
        "    plt.ylabel(data.columns[1])\n",
        "    plt.show()# Display the graphic\n",
        "\n"
      ],
      "metadata": {
        "id": "pfIOIBICbS5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convex Hull:\n",
        "Is good to see the data distribution and the detected outliers.\n",
        "But is not good with Mult Dimension"
      ],
      "metadata": {
        "id": "CjSIHEKIVEP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#IsolationForest\n",
        "def isolation_forest(data):\n",
        "\n",
        "  model = IsolationForest(contamination=0.05)  # Adjust contamination based on your dataset\n",
        "  model.fit(data) # Fit the model to the data\n",
        "  outliers = model.predict(data) # Predict anomalies (outliers)\n",
        "  anomaly_points = data[outliers == -1] # Extract the anomaly points\n",
        "  plt.scatter(data.iloc[:, 0], data.iloc[:, 1], s=50, c='b', label='Normal Data')\n",
        "  plt.scatter(anomaly_points.iloc[:, 0], anomaly_points.iloc[:, 1], s=50, c='r', label='Anomaly Points')\n",
        "  plt.legend()\n",
        "  plt.xlabel(data.columns[0])\n",
        "  plt.ylabel(data.columns[1])\n",
        "  plt.title('Isolation Forest')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "7IZEtbusJB0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Isolation Forest: s a fast and efficient algorithm for identifying outliers. It's especially useful for large datasets / Mult Dimension. The performace is not good"
      ],
      "metadata": {
        "id": "zND8fcB6Vtiq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Outliers\n",
        "def outliers(data):\n",
        "  plt.figure(figsize=(20, 10))\n",
        "  sns.boxplot(x=data[data.columns[0]], y=data[data.columns[1]], color='lightblue', width=0.6)\n",
        "  sns.stripplot(x=data[data.columns[0]], y=data[data.columns[1]], color='red', size=4, jitter=0.2)\n",
        "  plt.xlabel(data.columns[0])\n",
        "  plt.ylabel(data.columns[1])\n",
        "  plt.title(f\"Outliers\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "FT5iW2HqjZve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outliers: is good to analise the extremes for each feature and the performace is good to robust data and identify if have any anomaly, but could not be so efective and show some false positives"
      ],
      "metadata": {
        "id": "hT2XA7nPYfYu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bounding Box using ChatGPT\n",
        ">\n",
        "Rafael : can you do an exemple of bounding box with python.\n",
        ">\n",
        "ChatGPT: Generated\n",
        "```\n",
        "# Extract the feature columns\n",
        "# Calculate the minimum and maximum values for each feature\n",
        "# Define the bounding box\n",
        "```\n",
        "Rafael: how can i do a plot with the anwser in Bounding Box:\n",
        "```\n",
        "# Filter points within the bounding box\n",
        "# Create a scatter plot for all data points\n",
        "# Create a rectangle for the bounding box\n",
        "# Plot the\n",
        "# Highlight points within the bounding box\n",
        "```\n",
        "Rafael: ValueError: Can only compare identically-labeled Series objects\n",
        "---> 19     (X[\"temp_max\"] >= bounding_box[\"min_temp_max\n",
        ">\n",
        "ChatGPT: Fixed the Code, and return the atual code\n",
        "```\n",
        "This corrected code should allow you to create a plot with the bounding box and points within the box for your dataset. Make sure to replace \"dataset.csv\" with the path to your CSV file and adjust the column names as needed.\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LD8CE_FINK8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bounding Box from ChatGPT\n",
        "def bounding_box(data):\n",
        "    # Extract the feature columns\n",
        "    X = data\n",
        "\n",
        "    # Calculate the minimum and maximum values for each feature\n",
        "    min_vals = X.min()\n",
        "    max_vals = X.max()\n",
        "\n",
        "    # Define the bounding box using column names\n",
        "    bounding_box = {\n",
        "        f'min_{X.columns[0]}': min_vals[0],\n",
        "        f'max_{X.columns[0]}': max_vals[0],\n",
        "        f'min_{X.columns[1]}': min_vals[1],\n",
        "        f'max_{X.columns[1]}': max_vals[1]\n",
        "    }\n",
        "\n",
        "    # Filter points within the bounding box\n",
        "    points_within_box = X[\n",
        "        (X[X.columns[0]] >= bounding_box[f'min_{X.columns[0]}']) &\n",
        "        (X[X.columns[0]] <= bounding_box[f'max_{X.columns[0]}']) &\n",
        "        (X[X.columns[1]] >= bounding_box[f'min_{X.columns[1]}']) &\n",
        "        (X[X.columns[1]] <= bounding_box[f'max_{X.columns[1]}'])\n",
        "    ]\n",
        "\n",
        "    # Create a scatter plot for all data points\n",
        "    plt.scatter(X[X.columns[0]], X[X.columns[1]], c='b', label='Data Points')\n",
        "\n",
        "    # Create a rectangle for the bounding box\n",
        "    rectangle = plt.Rectangle(\n",
        "        (bounding_box[f'min_{X.columns[0]}'], bounding_box[f'min_{X.columns[1]}']),\n",
        "        bounding_box[f'max_{X.columns[0]}'] - bounding_box[f'min_{X.columns[0]}'],\n",
        "        bounding_box[f'max_{X.columns[1]}'] - bounding_box[f'min_{X.columns[1]}'],\n",
        "        color='r',\n",
        "        fill=False,\n",
        "        label='Bounding Box'\n",
        "    )\n",
        "\n",
        "    # Plot the rectangle\n",
        "    plt.gca().add_patch(rectangle)\n",
        "\n",
        "    # Highlight points within the bounding box\n",
        "    plt.scatter(points_within_box[X.columns[0]], points_within_box[X.columns[1]], c='g', label='Points in Bounding Box')\n",
        "\n",
        "    plt.xlabel(X.columns[0])\n",
        "    plt.ylabel(X.columns[1])\n",
        "    plt.legend()\n",
        "    plt.title(f\"Bounding Box and Data Points for {X.columns[0]} and {X.columns[1]}\")\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "Mzqfg0n67P8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bounding Box: is good to data exploration to find outliers in the data range but sensitive to the range of data values and may not capture outliers"
      ],
      "metadata": {
        "id": "SfsOd9i-Z-8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTqQuKZQLCF0",
        "outputId": "205848e0-5f06-47cd-e902-ab96ae6238c9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Periphery of a dataset is important for assessing a model's generalization, robustness, and its ability to handle unseen or extreme data points, including predictive modeling, anomaly detection, and fairness in machine learning."
      ],
      "metadata": {
        "id": "nhvmp8Wnipvy"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py39",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}