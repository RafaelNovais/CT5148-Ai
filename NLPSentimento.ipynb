{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZQtaGUMqvtWkgfxH3orsD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RafaelNovais/MasterAI/blob/master/NLPSentimento.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8OjDqrXJMZR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "resenha = pd.read_csv(\"dados/imdb-reviews-pt-br.csv\")\n",
        "resenha.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "treino, teste, classe_treino, classe_teste = train_test_split(resenha.text_pt,\n",
        "                                                              resenha.sentiment,\n",
        "                                                              random_state = 42)"
      ],
      "metadata": {
        "id": "-LvyBy4Ja3nn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "regressao_logistica = LogisticRegression()\n",
        "regressao_logistica.fit(treino, classe_treino)\n",
        "acuracia = regressao_logistica.score(teste, classe_teste)\n",
        "print(acuracia)"
      ],
      "metadata": {
        "id": "VjOTLYqoa552"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Negativa \\n\")\n",
        "print(resenha.text_pt[189])"
      ],
      "metadata": {
        "id": "rSfUYy9abD9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Positivo \\n\")\n",
        "print(resenha.text_pt[49002])"
      ],
      "metadata": {
        "id": "rc3Mng7DbG3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(resenha.sentiment.value_counts())\n",
        "resenha.head()"
      ],
      "metadata": {
        "id": "vgjwnQrVbIZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classificacao = resenha[\"sentiment\"].replace([\"neg\", \"pos\"], [0,1])\n",
        "resenha[\"classificacao\"] = classificacao\n",
        "resenha.head()\n",
        "resenha.tail()"
      ],
      "metadata": {
        "id": "l8_wPjiPbJ8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bag of Words"
      ],
      "metadata": {
        "id": "s4hLjWAvcaf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "texto = [\"Assisti um filme ótimo\", \"Assisti um filme ruim\"]\n",
        "\n",
        "vetorizar = CountVectorizer(lowercase=False)\n",
        "bag_of_words = vetorizar.fit_transform(texto)"
      ],
      "metadata": {
        "id": "RoZMo2lvbNZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vetorizar.get_feature_names()\n",
        "bag_of_words\n",
        "matriz_esparsa = pd.SparseDataFrame(bag_of_words,\n",
        "                      columns=vetorizar.get_feature_names())\n",
        "matriz_esparsa"
      ],
      "metadata": {
        "id": "i9LQBGYObWfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vetorizar = CountVectorizer(lowercase=False, max_features=50)\n",
        "bag_of_words = vetorizar.fit_transform(resenha.text_pt)\n",
        "print(bag_of_words.shape)"
      ],
      "metadata": {
        "id": "H1BWapd1beO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classificar_texto(texto, coluna_texto, coluna_classificacao):\n",
        "    vetorizar = CountVectorizer(lowercase=False, max_features=50)\n",
        "    bag_of_words = vetorizar.fit_transform(texto[coluna_texto])\n",
        "    treino, teste, classe_treino, classe_teste = train_test_split(bag_of_words,\n",
        "                                                              texto[coluna_classificacao],\n",
        "                                                              random_state = 42)\n",
        "    regressao_logistica = LogisticRegression(solver = \"lbfgs\")\n",
        "    regressao_logistica.fit(treino, classe_treino)\n",
        "    return regressao_logistica.score(teste, classe_teste)\n",
        "print(classificar_texto(resenha, \"text_pt\", \"classificacao\"))"
      ],
      "metadata": {
        "id": "Afv3dcANbhPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizando os dados com WordCloud."
      ],
      "metadata": {
        "id": "HhkhsHw3cTDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "todas_palavras = ' '.join([texto for texto in resenha.text_pt])\n",
        "\n",
        "nuvem_palvras = WordCloud(width= 800, height= 500,\n",
        "                          max_font_size = 110,\n",
        "                          collocations = False).generate(todas_palavras)"
      ],
      "metadata": {
        "id": "1ieD2NVFbjKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10,7))\n",
        "plt.imshow(nuvem_palvras, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A87T0ZgPbk_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nuvem_palavras_neg(texto, coluna_texto):\n",
        "    texto_negativo = texto.query(\"sentiment == 'neg'\")\n",
        "    todas_palavras = ' '.join([texto for texto in texto_negativo[coluna_texto]])\n",
        "\n",
        "    nuvem_palvras = WordCloud(width= 800, height= 500,\n",
        "                              max_font_size = 110,\n",
        "                              collocations = False).generate(todas_palavras)\n",
        "    plt.figure(figsize=(10,7))\n",
        "    plt.imshow(nuvem_palvras, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "iqomzKOCboZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nuvem_palavras_pos(texto, coluna_texto):\n",
        "    texto_positivo = texto.query(\"sentiment == 'pos'\")\n",
        "    todas_palavras = ' '.join([texto for texto in texto_positivo[coluna_texto]])\n",
        "\n",
        "    nuvem_palvras = WordCloud(width= 800, height= 500,\n",
        "                              max_font_size = 110,\n",
        "                              collocations = False).generate(todas_palavras)\n",
        "    plt.figure(figsize=(10,7))\n",
        "    plt.imshow(nuvem_palvras, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "7DKKaRSVbqTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nuvem_palavras_neg(resenha, \"text_pt\")\n",
        "nuvem_palavras_pos(resenha, \"text_pt\")"
      ],
      "metadata": {
        "id": "s3FmngZAbr_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenização e a bliblioteca NLTK."
      ],
      "metadata": {
        "id": "DSPJOkDZb0eu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "frase = [\"um filme bom\", \"um filme ruim\"]\n",
        "frequencia = nltk.FreqDist(frase)\n",
        "frequecia"
      ],
      "metadata": {
        "id": "38uxy308bvRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import tokenize\n",
        "\n",
        "frase = \"Bem vindo ao mundo do PLN!\"\n",
        "\n",
        "token_espaco = tokenize.WhitespaceTokenizer()\n",
        "token_frase = token_espaco.tokenize(frase)\n",
        "print(token_frase)"
      ],
      "metadata": {
        "id": "1D-ZrP6db4sQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_frase = token_espaco.tokenize(todas_palavras)\n",
        "frequencia = nltk.FreqDist(token_frase)\n",
        "df_frequencia = pd.DataFrame({\"Palavra\": list(frequencia.keys()),\n",
        "                                   \"Frequência\": list(frequencia.values())})"
      ],
      "metadata": {
        "id": "XIB3JEWFb7vo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_frequencia.nlargest(columns = \"Frequência\", n = 10)"
      ],
      "metadata": {
        "id": "7rMKm4-vb9NJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stop Words e Visualizacao"
      ],
      "metadata": {
        "id": "o3SIWKHfcCAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "def pareto(texto, coluna_texto, quantidade):\n",
        "    todas_palavras = ' '.join([texto for texto in texto[coluna_texto]])\n",
        "    token_frase = token_espaco.tokenize(todas_palavras)\n",
        "    frequencia = nltk.FreqDist(token_frase)\n",
        "    df_frequencia = pd.DataFrame({\"Palavra\": list(frequencia.keys()),\n",
        "                                   \"Frequência\": list(frequencia.values())})\n",
        "    df_frequencia = df_frequencia.nlargest(columns = \"Frequência\", n = quantidade)\n",
        "    plt.figure(figsize=(12,8))\n",
        "    ax = sns.barplot(data = df_frequencia, x = \"Palavra\", y = \"Frequência\", color = 'gray')\n",
        "    ax.set(ylabel = \"Contagem\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "pareto(resenha, \"text_pt\", 10)\n"
      ],
      "metadata": {
        "id": "KGesupZmcIf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "palavras_irrelevantes = nltk.corpus.stopwords.words(\"portuguese\")\n",
        "\n",
        "frase_processada = list()\n",
        "for opiniao in resenha.text_pt:\n",
        "    nova_frase = list()\n",
        "    palavras_texto = token_espaco.tokenize(opiniao)\n",
        "    for palavra in palavras_texto:\n",
        "        if palavra not in palavras_irrelevantes:\n",
        "            nova_frase.append(palavra)\n",
        "    frase_processada.append(' '.join(nova_frase))\n",
        "\n",
        "resenha[\"tratamento_1\"] = frase_processada"
      ],
      "metadata": {
        "id": "KV4al211cKzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classificar_texto(resenha, \"tratamento_1\", \"classificacao\")\n",
        "pareto(resenha,\"tratamento_1\", 10)"
      ],
      "metadata": {
        "id": "cOMUWO9hcNBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Otimizando a análise (Parte 2)\n",
        "\n"
      ],
      "metadata": {
        "id": "uLQeNa6qQ3qo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import tokenize\n",
        "\n",
        "frase = \"Olá mundo!\"\n",
        "token_pontuacao = tokenize.WordPunctTokenizer()\n",
        "token_frase = token_pontuacao.tokenize(frase)\n",
        "\n",
        "print(token_frase)"
      ],
      "metadata": {
        "id": "ckvT9401QVpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Melhorando a visualização do pareto\n",
        "\n",
        "from string import punctuation\n",
        "\n",
        "pontuacao = list()\n",
        "for ponto in punctuation:\n",
        "    pontuacao.append(ponto)\n",
        "\n",
        "pontuacao_stopwords = pontuacao + palavras_irrelevantes\n",
        "\n",
        "frase_processada = list()\n",
        "for opiniao in resenha[\"tratamento_1\"]:\n",
        "    nova_frase = list()\n",
        "    palavras_texto = token_pontuacao.tokenize(opiniao)\n",
        "    for palavra in palavras_texto:\n",
        "        if palavra not in pontuacao_stopwords:\n",
        "            nova_frase.append(palavra)\n",
        "    frase_processada.append(' '.join(nova_frase))\n",
        "\n",
        "resenha[\"tratamento_2\"] = frase_processada\n",
        "\n",
        "resenha.head()\n",
        "\n"
      ],
      "metadata": {
        "id": "CLyeWGciRKAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resenha.head()\n",
        "\n",
        "resenha[\"tratamento_2\"][0]\n",
        "\n",
        "pareto(resenha, \"tratamento_2\", 10)"
      ],
      "metadata": {
        "id": "igx4205lRsyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalização de textos\n",
        "\n",
        "import unidecode\n",
        "\n",
        "acentos = \"ótimo péssimo não é tão\"\n",
        "\n",
        "teste = unidecode.unidecode(acentos)\n",
        "print(teste)\n",
        "\n",
        "sem_acentos = [unidecode.unidecode(texto) for texto in resenha[\"tratamento_2\"]]\n",
        "\n",
        "sem_acentos[0]\n",
        "\n",
        "stopwords_sem_acento = [unidecode.unidecode(texto) for texto in pontuacao_stopwords]\n",
        "\n",
        "stopwords_sem_acento"
      ],
      "metadata": {
        "id": "zLus-VM3RyFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resenha[\"tratamento_3\"] = sem_acentos\n",
        "\n",
        "frase_processada = list()\n",
        "for opiniao in resenha[\"tratamento_3\"]:\n",
        "    nova_frase = list()\n",
        "    palavras_texto = token_pontuacao.tokenize(opiniao)\n",
        "    for palavra in palavras_texto:\n",
        "        if palavra not in stopwords_sem_acento:\n",
        "            nova_frase.append(palavra)\n",
        "    frase_processada.append(' '.join(nova_frase))\n",
        "\n",
        "resenha[\"tratamento_3\"] = frase_processada\n",
        "\n",
        "resenha.head()"
      ],
      "metadata": {
        "id": "4PRC6IDpSFSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acuracia_tratamento3 = classificar_texto(resenha, \"tratamento_3\", \"classificacao\")\n",
        "print(acuracia_tratamento3)"
      ],
      "metadata": {
        "id": "gf4LsaW_SKBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acuracia_teste\n",
        "\n",
        "nuvem_palavras_neg(resenha, \"tratamento_3\")\n",
        "\n",
        "nuvem_palavras_pos(resenha, \"tratamento_3\")\n",
        "\n",
        "pareto(resenha, \"tratamento_3\", 10)"
      ],
      "metadata": {
        "id": "v0yANwuQSL0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frase = \"O Thiago é o novo instrutor da Alura\"\n",
        "print(frase.lower())"
      ],
      "metadata": {
        "id": "HK_mSCu8SVTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frase_processada = list()\n",
        "for opiniao in resenha[\"tratamento_3\"]:\n",
        "    nova_frase = list()\n",
        "    opiniao = opiniao.lower()\n",
        "    palavras_texto = token_pontuacao.tokenize(opiniao)\n",
        "    for palavra in palavras_texto:\n",
        "        if palavra not in stopwords_sem_acento:\n",
        "            nova_frase.append(palavra)\n",
        "    frase_processada.append(' '.join(nova_frase))\n",
        "\n",
        "resenha[\"tratamento_4\"] = frase_processada\n",
        "\n",
        "resenha[\"text_pt\"][0]\n",
        "\n",
        "resenha[\"tratamento_4\"][0]"
      ],
      "metadata": {
        "id": "0mBgn_XuSYZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acuracia_tratamento4 = classificar_texto(resenha, \"tratamento_4\", \"classificacao\")\n",
        "print(acuracia_tratamento4)\n",
        "print(acuracia_tratamento3)\n",
        "\n",
        "nuvem_palavras_neg(resenha, \"tratamento_4\")\n",
        "\n",
        "nuvem_palavras_pos(resenha, \"tratamento_4\")\n",
        "\n",
        "pareto(resenha, \"tratamento_4\", 10)\n"
      ],
      "metadata": {
        "id": "kTqd1GmTSh1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evitando flexões e derivações nas palavras\n",
        "\n",
        "stemmer = nltk.RSLPStemmer()\n",
        "stemmer.stem(\"correria\")"
      ],
      "metadata": {
        "id": "vHPxxk0PSu_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frase_processada = list()\n",
        "for opiniao in resenha[\"tratamento_4\"]:\n",
        "    nova_frase = list()\n",
        "    palavras_texto = token_pontuacao.tokenize(opiniao)\n",
        "    for palavra in palavras_texto:\n",
        "        if palavra not in stopwords_sem_acento:\n",
        "            nova_frase.append(stemmer.stem(palavra))\n",
        "    frase_processada.append(' '.join(nova_frase))\n",
        "\n",
        "resenha[\"tratamento_5\"] = frase_processada"
      ],
      "metadata": {
        "id": "mxK7lMlASy6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acuracia_tratamento5 = classificar_texto(resenha, \"tratamento_5\", \"classificacao\")\n",
        "print(acuracia_tratamento5)\n",
        "print(acuracia_tratamento4)\n",
        "\n",
        "\n",
        "nuvem_palavras_neg(resenha, \"tratamento_5\")\n",
        "\n",
        "nuvem_palavras_pos(resenha,\"tratamento_5\")\n",
        "\n",
        "pareto(resenha, \"tratamento_5\", 10)"
      ],
      "metadata": {
        "id": "SoRtw8DES19B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}