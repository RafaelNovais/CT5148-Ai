{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMbd7RUKats5ZGFjXI1yeDI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RafaelNovais/MasterAI/blob/master/QueryTerms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1\n",
        "\n",
        "The algorithm for suggesting query augmentation terms is designed to address the dual goals of relevance and diversity. It aims to provide users with a rich set of terms to expand their query while capturing different facets of the underlying information need. Below, I explain the design and rationale behind each step of the process.\n",
        "\n",
        "The algorithm begins with the user query and the top N documents retrieved as relevant by an Information Retrieval system. Preprocessing these documents is crucial to ensure that only meaningful terms are considered. Tokenization breaks text into individual words, while stopword removal eliminates common words that do not contribute to meaning. Stemming or lemmatization reduces words to their root forms, helping group variations together.\n",
        "\n",
        "Candidate terms are extracted from the preprocessed documents using TF-IDF. TF-IDF ranks terms based on their frequency in the top N documents while penalizing terms common across all documents. This scoring highlights terms that are both significant within the retrieved set and less generic, ensuring that suggestions are specific to the query context. Original query terms are excluded from the candidate list, as the focus is on generating additional terms.\n",
        "\n",
        "To ensure the terms are analyzed in a meaningful way, they are converted into vector representations using pre-trained word embeddings. These embeddings capture semantic relationships between words based on their contextual usage in large corpora. For example, \"global warming\" and \"climate change\" would have similar vector representations.\n",
        "\n",
        "A key requirement is diversity in suggestions, ensuring that various aspects of the query intent are covered. Clustering algorithms are applied to group semantically similar terms. Each cluster represents a distinct theme or facet related to the query. The number of clusters can be determined dynamically, balancing granularity and usability.From each cluster, the algorithm selects representative terms, ensuring diversity by picking terms closest to the cluster centroid. This avoids redundancy while preserving relevance within the thematic grouping.Candidate terms are scored based on their relevance to the query and their co-occurrence with query terms in the retrieved documents. A penalty is applied to similar terms to maintain diversity."
      ],
      "metadata": {
        "id": "WjXZHw20j9V4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "z6wHAu9ifv82"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from gensim.models import Word2Vec\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.data.path = []\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "dsaJEvaMga6q",
        "outputId": "d16ae8cd-e392-4717-a4f3-bd6fbe04671c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_documents(documents):\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    processed_docs = []\n",
        "    for doc in documents:\n",
        "        tokens = word_tokenize(doc.lower())\n",
        "        tokens = [t for t in tokens if t.isalnum() and t not in stop_words]\n",
        "        processed_docs.append(tokens)\n",
        "    return processed_docs"
      ],
      "metadata": {
        "id": "RA14_UMsgGkG"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_candidate_terms(docs, query_terms):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform([' '.join(doc) for doc in docs])\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    scores = tfidf_matrix.sum(axis=0).A1\n",
        "    candidate_terms = [\n",
        "        feature_names[i] for i in scores.argsort()[::-1] if feature_names[i] not in query_terms\n",
        "    ]\n",
        "    return candidate_terms"
      ],
      "metadata": {
        "id": "_0Pnop8PgI5M"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cluster_terms(candidate_terms, docs):\n",
        "    model = Word2Vec(docs, vector_size=100, min_count=1, workers=4)\n",
        "    term_vectors = [model.wv[term] for term in candidate_terms if term in model.wv]\n",
        "    kmeans = KMeans(n_clusters=5)\n",
        "    clusters = kmeans.fit_predict(term_vectors)\n",
        "    cluster_terms = {}\n",
        "    for idx, term in enumerate(candidate_terms):\n",
        "        cluster = clusters[idx]\n",
        "        if cluster not in cluster_terms:\n",
        "            cluster_terms[cluster] = []\n",
        "        cluster_terms[cluster].append(term)\n",
        "    return cluster_terms"
      ],
      "metadata": {
        "id": "XRjh4UIugLFd"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def suggest_query_terms(query, documents, top_n=5):\n",
        "    processed_docs = preprocess_documents(documents)\n",
        "    query_terms = query.lower().split()\n",
        "    candidate_terms = get_candidate_terms(processed_docs, query_terms)\n",
        "    clusters = cluster_terms(candidate_terms, processed_docs)\n",
        "    suggestions = [terms[:top_n] for cluster, terms in clusters.items()]\n",
        "    return suggestions\n"
      ],
      "metadata": {
        "id": "8C-SPjwXgPf-"
      },
      "execution_count": 29,
      "outputs": []
    }
  ]
}