{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMt3L0GKNCNzXpn9Hr8nl0M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RafaelNovais/MasterAI/blob/master/MindMapAIEthics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Week 1\n",
        "\n",
        "**Embedded values (Nissenbaum) Valores incorporados**\n",
        "\n",
        "“the design and operation of computer system has moral consequences and therefore should be subjected to ethical analysis” (Brey 2009) it is possible to identify tendencies regarding promotion of particular values and norms for example, computer programs can be supportive of privacy, freedom of information, property rights or go against the realization of these values.\n",
        "\n",
        "Problem is when the questions about ethics just appear in the End, is diffulct to control.\n",
        "\n",
        "Technologies are not neutral\n",
        "\n",
        "E.g. common view of engineering as neutral service-provider for client who is responsible for ethical use Importance because: information technologies often not transparent to users, i.e. users cannot easily investigate technologies and their biases\n",
        "\n",
        "Like Guns doesn't kill people, people kill people.\n",
        "\n",
        "**Biases in computer systems Preconceitos**\n",
        "\n",
        "Definition: A system that systematically and unfairly discriminates against certain individuals or groups in favour of others E.g. credit rating system that instead of focusing on actual credit record bases its assessment on address or ethnic surname Predictive policing targeting minority populations that will disproportionately be affected by police scrutiny (e.g. minor drug possession charges when searches) Criminal justice parole assessment systems that discriminate against minorities Particularly problematic if no possibility of appeal/correction or validation and if a system becomes a standard (monopolist) in the field and no alternatives exist\n",
        "\n",
        "Pre-existing bias: computer system is designed by somebody who holds these biases and encodes them in the technology\n",
        "Technical bias: limitations of computer technology might lead to bias, or problems in achieving transition of concepts into technical realisation\n",
        "Emergent bias: arising from changing contexts of use, e.g. mismatch between users and system design\n",
        "\n",
        "**Accessibility as a value concern**\n",
        "\n",
        "Equal inclusion of persons with different capabilities and backgrounds as important value.\n",
        "\n",
        "Part of human rights requirements, e.g. Convention of the Rights of Persons with Disabilities.\n",
        "\n",
        "Goal of achieving “Universal design“, i.e. design solutions that facilitate accessibility for different disabilities, and other potential forms of exclusion Example: flight booking with visual impairment, “racist soap dispenser\n",
        "\n",
        "\n",
        "**Procedure of value sensitive design**\n",
        "\n",
        "\n",
        "Value-sensitive design refers to a theoretical approach that identifies and incorporates stakeholders’ values into the design process of a new technology. Especially when new technologies are developed that open up new ways of impacting human life. Proactive approach rather than reactive approach, i.e. identify issues before they occur rather than having to troubleshoot later\n",
        "\n",
        "Conceptual: identifying relevant values and who might be relevant stakeholders Using philosophical and social science theory and existing findings\n",
        "Empirical: actively engaging stakeholders’ perspectives to identify variation and commonalities\n",
        "Method: observation, surveys, interviews, focus groups Technical: proactive design to support values identified in the conceptual investigation; identify how existing technological properties support or hinder realisation of values\n",
        "\n",
        "**Trustworthiness**\n",
        "\n",
        "based on stable and effective patterns of behaviour and attitudes, indicating that the person trusted has taken our goals on board and is motivated to act in our interests.\n",
        "\n",
        "* Competence (can do)\n",
        "* Reliability (will do)\n",
        "* Dispositional (will adapt/respond adequately)\n",
        "* Interest in maintenance of positive relationship (wants to do)\n",
        "* Moral response to dependency by others (doing for moral reasons)\n",
        "* Conveys own trustworthiness proactively and appropriately to those in position of dependence (relational)\n",
        "\n",
        "**Seven key requirements for Trustworthy AI**\n",
        "\n",
        "1. human agency and oversight\n",
        "2. technical robustness and safety\n",
        "3. privacy and data governance\n",
        "4. transparency\n",
        "5. diversity, non-discrimination and fairness\n",
        "6. environmental and societal well-being\n",
        "7. accountability"
      ],
      "metadata": {
        "id": "dTXUtFfKKEYD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Week 2\n",
        "\n",
        "**The privacy paradox**\n",
        "\n",
        "We claim to care about privacy, but our actions don’t match that claim\n",
        "\n",
        "Option 1: behaviour is the strongest indicator of values, i.e. if I don’t act in line with the most privacy respecting choices that means I don’t value privacy\n",
        "\n",
        "Option 2: behaviour distortion: people act out of line with privacy because they are subject to\n",
        "distorting factors that make them act irrationally\n",
        "\n",
        "Option 3 (Solove): there is no paradox, general attitudes (representing attitudes about the value of privacy) are different from specific  behaviour (representing decision-making on risk in specific situations/contexts, underpinned by assumptions how likely harmful use of data in this situation is)\n",
        "\n",
        "Privacy is not \"either-or\"\n",
        "\n",
        "Often assumption of dichotomy between privacy and publicity, reflected in legal norms regarding private vs public information\n",
        "but not doing justice to the specific features of informational norms in a given context\n",
        "particularly problematic in the field of digital data practices,\n",
        "e.g.:\n",
        "Private information can be deduced from specific public characteristics\n",
        "Publicly available information can become more sensitive when it becomes accessible beyond the local\n",
        "Importance of having an analytic framework that allows more ethically differentiated reflection on the impact of new and emerging data practices\n",
        "\n",
        "\n",
        "\n",
        "**4 Core Claims (Nissenbaum 2019)**\n",
        "1. Privacy is the appropriate flow of personal information\n",
        "2. Appropriate flows conform with contextual informational norms or “privacy norms”\n",
        "3. Five Parameters Define Privacy (Contextual Informational) Norms:Subject, Sender, Recipient, Information Type, and Transmission Principle\n",
        "\n",
        "4. The Ethical Legitimacy of Privacy Norms is Evaluated in Terms of:\n",
        "  * Interests of Affected Parties.\n",
        "  * Ethical and Political Values.\n",
        "  * Contextual Functions, Purposes, and Values.\n",
        "  \n",
        "\n",
        "**The notice and consent paradigm**\n",
        "* The notice and consent paradigm Service users whose personal data is being processed need to be provided with a privacy policy (notice)\n",
        "* The privacy policy outlines in detail what data is being processed, how and for what purposes.\n",
        "* The consumer needs to agree to this policy before such information can be collected (consent)\n",
        "\n",
        "\n",
        "**Ethical responsibilities in informed consent**\n",
        "* Informed consent as core ethical concept in professional ethics, healthcare ethics and research ethics\n",
        "* Transactional concept, i.e. about interaction between professional/service provider and client/service user\n",
        "* Ethical acknowledgement of asymmetry and power differential between highly informed professional or provider and lay person who generally has less knowledge and power and is often dependent on the service provider\n",
        "* Ethical responsibility of professional or provider to facilitate client in making a meaningful decision (and not abuse powerful position)\n",
        "\n",
        "**Elements of Informed Consent**\n",
        "\n",
        "*Threshold elements\n",
        " 1. Competence / capacity: The person who consents is not limited in their decision-making capacity and can make meaningful, thought-out decisions\n",
        " 2. Voluntariness: the person who consents is not coerced, unduly pressured or inappropriately induced to consent\n",
        "\n",
        "*Information elements\n",
        " 1. Information: the service provider provides selected relevant information that pertains to the decision whether to consent in an understandable format\n",
        " 2. Understanding: the service provider ensures that the information provided was understood adequately by the person who consents\n",
        "\n",
        " *Decision Elements\n",
        " 1. Decision: based on the information provided during the process, the person who consents decides which option to chose\n",
        " 2. Authorization: the person who consents authorizes the service provider to go ahead with the outlined interventionsor practices\n",
        "\n",
        "**Dark patterns**\n",
        "\n",
        "* Investigated in human computer interaction, as “dark side” of persuasive design\n",
        "* Definition: “instances where designers use their knowledge of human behavior (e.g., psychology) and the desires of end users to implement deceptive functionality that is not in the user’s best interest” (Gray et al. 2018)\n",
        "* Nouwens et al. (2020) on consent notice presentation and user impact: design choices are frequently not GDPR compliant and have significant impact on user choices\n",
        "\n",
        "over 50% of sites in 2019 post GDPR used dark patterns, 50% did not have “reject all” button; only 12% of those with “reject all” button had “accept all” and “reject all” on same page\n",
        "only 1% of choices in user study were a specific purpose selection (rather than accept all or reject all)"
      ],
      "metadata": {
        "id": "py3VGYALLXsW"
      }
    }
  ]
}