{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZh//GMREdXvJNR88mPBcB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RafaelNovais/MasterAI/blob/master/DeepLearningMindMap.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjdVq7_zyxgJ",
        "outputId": "d2be7272-c4f0-4de0-a130-b601ec334320"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 0.5758\n",
            "Epoch 2/20, Loss: 0.4979\n",
            "Epoch 3/20, Loss: 0.4441\n",
            "Epoch 4/20, Loss: 0.4048\n",
            "Epoch 5/20, Loss: 0.3747\n",
            "Epoch 6/20, Loss: 0.3506\n",
            "Epoch 7/20, Loss: 0.3310\n",
            "Epoch 8/20, Loss: 0.3145\n",
            "Epoch 9/20, Loss: 0.3006\n",
            "Epoch 10/20, Loss: 0.2885\n",
            "Epoch 11/20, Loss: 0.2779\n",
            "Epoch 12/20, Loss: 0.2686\n",
            "Epoch 13/20, Loss: 0.2602\n",
            "Epoch 14/20, Loss: 0.2526\n",
            "Epoch 15/20, Loss: 0.2458\n",
            "Epoch 16/20, Loss: 0.2396\n",
            "Epoch 17/20, Loss: 0.2339\n",
            "Epoch 18/20, Loss: 0.2286\n",
            "Epoch 19/20, Loss: 0.2238\n",
            "Epoch 20/20, Loss: 0.2192\n",
            "Predictions: [1 0]\n"
          ]
        }
      ],
      "source": [
        "#LOGISTIC REGRESSION\n",
        "import numpy as np\n",
        "\n",
        "class LogisticRegression:\n",
        "    def __init__(self, learning_rate=0.01, num_epochs=100, batch_size=1):\n",
        "        \"\"\"\n",
        "        Logistic Regression model using Stochastic Gradient Descent.\n",
        "\n",
        "        Parameters:\n",
        "        - learning_rate: Learning rate for gradient descent.\n",
        "        - num_epochs: Number of epochs to train.\n",
        "        - batch_size: Number of samples per batch for SGD.\n",
        "        \"\"\"\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_epochs = num_epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        \"\"\"Apply sigmoid function.\"\"\"\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def forward_pass(self, X):\n",
        "        \"\"\"Compute the linear combination and apply sigmoid.\"\"\"\n",
        "        return self.sigmoid(np.dot(X, self.weights) + self.bias)\n",
        "\n",
        "    def compute_loss(self, y, y_pred):\n",
        "        \"\"\"Compute binary cross-entropy loss.\"\"\"\n",
        "        epsilon = 1e-15  # Prevent log(0)\n",
        "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "        return -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Train the logistic regression model using SGD.\n",
        "\n",
        "        Parameters:\n",
        "        - X: Feature matrix of shape (n_samples, n_features).\n",
        "        - y: Labels vector of shape (n_samples,).\n",
        "        \"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            indices = np.arange(n_samples)\n",
        "            np.random.shuffle(indices)\n",
        "            X = X[indices]\n",
        "            y = y[indices]\n",
        "\n",
        "            for start in range(0, n_samples, self.batch_size):\n",
        "                end = start + self.batch_size\n",
        "                X_batch = X[start:end]\n",
        "                y_batch = y[start:end]\n",
        "\n",
        "                y_pred = self.forward_pass(X_batch)\n",
        "\n",
        "                dw = np.dot(X_batch.T, (y_pred - y_batch)) / len(y_batch)\n",
        "                db = np.sum(y_pred - y_batch) / len(y_batch)\n",
        "\n",
        "                self.weights -= self.learning_rate * dw\n",
        "                self.bias -= self.learning_rate * db\n",
        "\n",
        "            y_pred_epoch = self.forward_pass(X)\n",
        "            loss = self.compute_loss(y, y_pred_epoch)\n",
        "            print(f\"Epoch {epoch + 1}/{self.num_epochs}, Loss: {loss:.4f}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict binary labels for input data.\n",
        "\n",
        "        Parameters:\n",
        "        - X: Feature matrix of shape (n_samples, n_features).\n",
        "\n",
        "        Returns:\n",
        "        - Binary predictions (0 or 1).\n",
        "        \"\"\"\n",
        "        y_pred = self.forward_pass(X)\n",
        "        return (y_pred >= 0.5).astype(int)\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Generate dummy data\n",
        "    np.random.seed(42)\n",
        "    X = np.random.randn(100, 2)\n",
        "    y = (np.dot(X, [1.5, -2.0]) + 0.5 > 0).astype(int)\n",
        "\n",
        "    # Initialize and train model\n",
        "    model = LogisticRegression(learning_rate=0.1, num_epochs=20, batch_size=10)\n",
        "    model.fit(X, y)\n",
        "\n",
        "    # Predict on new data\n",
        "    X_test = np.array([[0.5, -1.0], [-1.5, 2.0]])\n",
        "    predictions = model.predict(X_test)\n",
        "    print(\"Predictions:\", predictions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Classic ML\n",
        "\n",
        "1 - Classification  Predict a function/classe\n",
        "\n",
        "2 - Regression      Predict a number/specific value\n",
        "\n",
        "3 - Clustering      Predict or create a Group based in details or different details\n",
        "\n",
        "4 - Co-trainig  is a cluster with classification, you can group based an specific object/details\n",
        "\n",
        "5 - Relationship Discovery - find association like Chips and ketchup\n",
        "\n",
        "6 - Reinforcement Learning - Positive Reinforcement like rewards, Negative Reinforcement Punishment , the agent need explore the envirament\n",
        "\n",
        "\n",
        "\n",
        "**Ensembles: Basic Idea** AGRUPAMENTO/COMIBINAR DE PROCESSAMENTOS/Modelos PARA DIVERSIFICAR PROCESAR VARIAS VEZES E AGRUPAR OS RESULTADOS \"Muitos sao mais inteligentes que alguns\" - sing your data, construct multiple classifiers,Combine the decisions to arrive at final decision\n",
        "\n",
        "* Bagging - Bootstrap Aggregation, decrease the variance by generating multiple data sets of the same size as the original data set - Processa Independente/Paralelo e depois combina\n",
        "  * Simulates the existence of multiple data sets\n",
        "  * Bagging works by reducing the variance through averaging/voting over multiple classifiers created with different training subsets\n",
        "  * orks well if the classification algorithm is unstable\n",
        "\n",
        "* Boosting – Uses subsets of the original data set to produce averagely performing models weighted towards “harder” problems. It then “boosts” the results through a vote. Processo em sequencia, depois reprocessa N vezes ate classificacao final\n",
        "  * Also uses voting/averaging\n",
        "  * Weights models according to performance\n",
        "  * Boosting generates a series of classifiers\n",
        "  * Their output is combined to produce a weighted vote\n",
        "\n",
        "* Random Forests\n",
        "  * CART is a Decision Tree classifier,Create a ‘forest’ of trees as an ensemble\n",
        "  * Works like bagging, except – Decision tree is base learner\n",
        "\n",
        "* Stacking – Combines the results from multiple heterogeneous classifiers to improve classification. A voting scheme is not applied, instead a linear alg is\n",
        "applied to make the classification - Processa Paralelo depois combina Meta-Modelo heterogeneous\n",
        "  * Stacking, a.k.a Stacked Generalisation, is a family of methods for heterogenous – Classifiers are built using different classification algorithms\n",
        "  * Because of their different characteristics, stacking seeks to combine them in a way that is more sophisticated than a simple vote\n",
        "\n",
        "\n",
        "\n",
        "# Neural Networks\n",
        "\n",
        "Highly parallel computation\n",
        "\n",
        "  – Weights: positive or negative\n",
        "\n",
        "  – Activation: 0/1, soft threshold; nonlinear; differentiable Commonly: sigmoid function\n",
        "\n",
        "  – Output: \"Squashed\" linear function of inpu\n",
        "\n",
        "\n",
        "**Fully Connected Feed-Forward Neural Network**\n",
        "\n",
        "* Also known as a Multi-Layer Perceptron\n",
        "* Simplest architecture, widely used\n",
        "* Neurons connected in layers\n",
        "    – Family of functions parameterised by weights\n",
        "    – No internal state\n",
        "* High-dimensional non-linear interpolation\n",
        "* Network is a distributed model of the data\n",
        "\n",
        "\n",
        "\n",
        "# Gradient Descent with Backpropagation\n",
        "\n",
        "* orward propagation\n",
        "* Backpropagation Step\n",
        "  * Output Layer\n",
        "  * Hidden Layers\n",
        "* Gradient Descent Update Step\n",
        "\n",
        "\n",
        "# Common Activation Functions\n",
        "\n",
        "* Logistic\n",
        "* Tanh\n",
        "* ReLU\n",
        "* Leaky ReLU\n",
        "\n",
        "\n",
        "* **For classification tasks, we will use the same average log loss cost function that we saw last week for logistic regression**\n",
        "\n",
        "*\n",
        "\n",
        "# Deep Neural Networks\n",
        "\n",
        "* NNs with more than a small number of hidden layers Varias Camadas de processamento\n",
        "  * Generally large in scale: many input nodes; many hidden layers; many nodes per hidden layer; large amounts of training data\n",
        "\n",
        "\n",
        "**High-Level Perspectives**\n",
        "* Practical perspective:\n",
        "  * Deep architectures achieve many of the same things as shallow ones, but more efficiently, particularly for perception tasks(vision/sound)\n",
        "  * Basic training techniques were not effective, so we needed to find ways of getting it to work\n",
        "* Connectionist programming perspective:\n",
        "  * Deep learning provides an integrated approach to feature engineering and learning, all within a connectionist architecture\n",
        "  * Nodes in early layers can be considered functions/subroutines that are re-used in later ones\n",
        "  * Convolutional NNs extend this idea further\n",
        "\n",
        "* Structured Data\n",
        "  * typically organised in a table\n",
        "* Unstructured Data\n",
        "  * Photos, Movies, Audio, Documents\n",
        "* Multi-Class Classification - Represent classes as 3 or + outputs\n",
        "  * The Softmax function replaces the standard Sigmoid function used in binary classification. It rescales the z values so that the ො𝑦 values sum to 1, as required for a probability distribution.\n",
        "  \n",
        "* Methods to Avoid Overfitting\n",
        "  * Data Augmentation - The best way to improve generalisation on a ML model is to train it with more data\n",
        "  * Early Stopping - In early stages of NN training, when it is far from converging, overfitting is never an issue, but it can become an issue as training proceeds, particularly if network is complex relative to dataset size\n",
        "  * L2 regularisation - The idea behind regularization is that we add an extra penalty term to the cost function to penalise more complex networks  Python: numpy.linag.norm\n",
        "\n",
        "\n",
        "# Training Algorithms\n",
        "* Mini-Batch Gradient Descent\n",
        " * Divide full dataset into mini batches\n",
        " * Loop over mini-batches, and do one iteration of the training algorithm on 1 mini-batch\n",
        "\n",
        "\n",
        "* Backprop with Momentum\n",
        "  * Each parameter should be able to change at a rate appropriate to itself, rather than there being one fixed learning rate\n",
        "  * The previous changes in parameter values should influence the current direction of updates\n",
        "  * To achieve this, for each parameter, compute an exponentially weighted moving average of previous gradients, and use that to update the parameter\n",
        "\n",
        "* RMSprop Root Means Square\n",
        "  * Another adaptive learning rate algorithm, proposed by Geoff Hinton, and strongly related to an earlier one called AdaGrad,\n",
        "  * For each parameter, keep a moving average of its squared gradient\n",
        "  * When updating, divide the current gradient by the square root of the average squared gradient\n",
        "\n",
        "* Adam Optimiser = Adaptive Moment Estimation\n",
        "  * Essentially, combine the ideas from Momentum and RMSprop: consider both velocity and acceleration terms\n",
        "\n",
        "* Convolutional Networks\n",
        "  * Building on previous idea, introduce a set of shared weights and biases for each field\n",
        "  * Usually have multiple convolutional layers in a CNN\n",
        "\n",
        "\n",
        "* Model Re-Use\n",
        "  * Pre-Training\n",
        "  * Transfer Learning\n",
        "\n",
        "* Convolutions ???\n",
        "* Convolutions on Multi-Channel Inputs ??\n",
        "* Multiple Feature Detectors\n",
        "\n",
        "\n",
        "**CNNs Incorporate Important Ideas that can Help Learning**\n",
        "* Sparse Connectivity\n",
        "  * Rather than having fully-connected layers, only some units in one layer\n",
        "  are derived from values of some units in previous layer\n",
        "  * Reduces number of weights, reducing overfitting risk, computational cost\n",
        "  and training demands\n",
        "* Parameter Sharing\n",
        "  * Rather than learning a separate parameter for\n",
        "  each connection, learn shared parameters that\n",
        "  represent specific operations\n",
        "  * Again, reduces number of weights\n",
        "* Equivariant Representations\n",
        "  * If you apply a transformation to input and then put\n",
        "  it through conv layer, equivalent to putting it through\n",
        "  conv layer and then applying the transformation\n",
        "  * For CNNs, true of translation operations specifically,\n",
        "  but not others such as rotations"
      ],
      "metadata": {
        "id": "Pci65G941JLb"
      }
    }
  ]
}