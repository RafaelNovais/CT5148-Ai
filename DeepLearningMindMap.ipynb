{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOuZ95s5vNW4FJegp7CDxt/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RafaelNovais/MasterAI/blob/master/DeepLearningMindMap.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjdVq7_zyxgJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Classic ML\n",
        "\n",
        "1 - Classification  Predict a function/classe\n",
        "\n",
        "2 - Regression      Predict a number/specific value\n",
        "\n",
        "3 - Clustering      Predict or create a Group based in details or different details\n",
        "\n",
        "4 - Co-trainig  is a cluster with classification, you can group based an specific object/details\n",
        "\n",
        "5 - Relationship Discovery - find association like Chips and ketchup\n",
        "\n",
        "6 - Reinforcement Learning - Positive Reinforcement like rewards, Negative Reinforcement Punishment , the agent need explore the envirament\n",
        "\n",
        "\n",
        "\n",
        "**Ensembles: Basic Idea** AGRUPAMENTO DE PROCESSAMENTOS PARA DIVERSIFICAR PROCESAR VARIAS VEZES E AGRUPAR OS RESULTADOS - sing your data, construct multiple classifiers,Combine the decisions to arrive at final decision\n",
        "\n",
        "* Bagging - Bootstrap Aggregation, decrease the variance by generating multiple data sets of the same size as the original data set\n",
        "  * Simulates the existence of multiple data sets\n",
        "  * Bagging works by reducing the variance through averaging/voting over multiple classifiers created with different training subsets\n",
        "  * orks well if the classification algorithm is unstable\n",
        "\n",
        "* Boosting – Uses subsets of the original data set to produce averagely performing models weighted towards “harder” problems. It then “boosts” the results through a vote.\n",
        "  * Also uses voting/averaging\n",
        "  * Weights models according to performance\n",
        "  * Boosting generates a series of classifiers\n",
        "  * Their output is combined to produce a weighted vote\n",
        "\n",
        "* Random Forests\n",
        "  * CART is a Decision Tree classifier,Create a ‘forest’ of trees as an ensemble\n",
        "  * Works like bagging, except – Decision tree is base learner\n",
        "\n",
        "* Stacking – Combines the results from multiple heterogeneous classifiers to improve classification. A voting scheme is not applied, instead a linear alg is\n",
        "applied to make the classification\n",
        "  * Stacking, a.k.a Stacked Generalisation, is a family of methods for heterogenous – Classifiers are built using different classification algorithms\n",
        "  * Because of their different characteristics, stacking seeks to combine them in a way that is more sophisticated than a simple vote"
      ],
      "metadata": {
        "id": "Pci65G941JLb"
      }
    }
  ]
}